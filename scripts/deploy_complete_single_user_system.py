#!/usr/bin/env python3
"""
Complete deployment of Cherry AI with single-user optimizations
Integrates authentication, monitoring, and performance optimizations
"""

import os
import sys
import subprocess
import asyncio
import json
from pathlib import Path
from datetime import datetime
import time

class CompleteSystemDeployment:
    def __init__(self):
        self.base_dir = Path("/root/cherry_ai-main")
        self.deployment_time = datetime.now()
        self.deployment_log = []
        
    def log(self, message: str, level: str = "INFO"):
        """Log deployment messages"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        print(log_entry)
        self.deployment_log.append(log_entry)
    
    async def run(self):
        """Execute complete deployment"""
        self.log("üöÄ Starting Complete Single-User Cherry AI Deployment")
        self.log("=" * 60)
        
        try:
            # Phase 1: Pre-deployment checks
            await self.pre_deployment_checks()
            
            # Phase 2: Deploy authentication system
            await self.deploy_authentication()
            
            # Phase 3: Deploy monitoring system
            await self.deploy_monitoring()
            
            # Phase 4: Update API with all integrations
            await self.integrate_api()
            
            # Phase 5: Deploy infrastructure
            await self.deploy_infrastructure()
            
            # Phase 6: Run comprehensive tests
            await self.run_comprehensive_tests()
            
            # Phase 7: Generate deployment report
            await self.generate_deployment_report()
            
            self.log("‚úÖ Deployment completed successfully!", "SUCCESS")
            
        except Exception as e:
            self.log(f"‚ùå Deployment failed: {e}", "ERROR")
            await self.rollback()
            sys.exit(1)
    
    async def pre_deployment_checks(self):
        """Perform pre-deployment validation"""
        self.log("\nüìã Phase 1: Pre-deployment Checks")
        
        checks = {
            "Docker": self.check_docker(),
            "Python": self.check_python(),
            "Network": self.check_network(),
            "Disk Space": self.check_disk_space(),
            "Environment": self.check_environment()
        }
        
        all_passed = True
        for check, result in checks.items():
            if result:
                self.log(f"  ‚úì {check}: OK")
            else:
                self.log(f"  ‚úó {check}: FAILED", "ERROR")
                all_passed = False
        
        if not all_passed:
            raise Exception("Pre-deployment checks failed")
    
    def check_docker(self) -> bool:
        """Check Docker availability"""
        try:
            result = subprocess.run(
                ["docker", "--version"],
                capture_output=True,
                text=True
            )
            return result.returncode == 0
        except:
            return False
    
    def check_python(self) -> bool:
        """Check Python version"""
        return sys.version_info >= (3, 8)
    
    def check_network(self) -> bool:
        """Check network connectivity"""
        try:
            import socket
            socket.create_connection(("8.8.8.8", 53), timeout=3)
            return True
        except:
            return False
    
    def check_disk_space(self) -> bool:
        """Check available disk space"""
        import shutil
        stat = shutil.disk_usage("/")
        # Require at least 5GB free
        return stat.free > 5 * 1024 * 1024 * 1024
    
    def check_environment(self) -> bool:
        """Check required environment variables"""
        # For single-user, we'll generate if missing
        return True
    
    async def deploy_authentication(self):
        """Deploy single-user authentication system"""
        self.log("\nüîê Phase 2: Deploying Authentication System")
        
        # Generate API key if not exists
        api_key = os.getenv("cherry_ai_API_KEY")
        if not api_key:
            import secrets
            api_key = secrets.token_urlsafe(48)
            self.log(f"  ‚úì Generated API key: {api_key[:10]}...")
            
            # Update .env file
            env_file = self.base_dir / ".env"
            if env_file.exists():
                with open(env_file, 'a') as f:
                    f.write(f"\ncherry_ai_API_KEY={api_key}\n")
            
            # Set in environment for current process
            os.environ["cherry_ai_API_KEY"] = api_key
        else:
            self.log("  ‚úì Using existing API key")
        
        # Set context
        if not os.getenv("cherry_ai_CONTEXT"):
            os.environ["cherry_ai_CONTEXT"] = "development"
            self.log("  ‚úì Set context to development")
        
        self.log("  ‚úì Authentication system configured")
    
    async def deploy_monitoring(self):
        """Deploy performance monitoring system"""
        self.log("\nüìä Phase 3: Deploying Monitoring System")
        
        # Create monitoring configuration
        monitoring_config = {
            "enabled": True,
            "context": os.getenv("cherry_ai_CONTEXT", "development"),
            "metrics": {
                "system": ["cpu", "memory", "disk", "network"],
                "application": ["requests", "errors", "latency"],
                "custom": ["workflows", "agents", "cache"]
            },
            "thresholds": {
                "cpu_percent": 85,
                "memory_percent": 80,
                "response_time_ms": 200,
                "error_rate": 0.01
            },
            "export": {
                "enabled": True,
                "interval_minutes": 60,
                "retention_days": 7
            }
        }
        
        config_path = self.base_dir / "config" / "monitoring.json"
        config_path.parent.mkdir(exist_ok=True)
        
        with open(config_path, 'w') as f:
            json.dump(monitoring_config, f, indent=2)
        
        self.log("  ‚úì Monitoring configuration created")
        
        # Create monitoring service
        await self.create_monitoring_service()
    
    async def create_monitoring_service(self):
        """Create systemd service for monitoring"""
        service_content = """[Unit]
Description=Cherry AI Performance Monitor
After=docker.service
Requires=docker.service

[Service]
Type=simple
Restart=always
RestartSec=10
WorkingDirectory=/root/cherry_ai-main
Environment="PYTHONPATH=/root/cherry_ai-main"
ExecStart=/usr/bin/python3 -m mcp_server.monitoring.service
StandardOutput=append:/var/log/cherry_ai-monitor.log
StandardError=append:/var/log/cherry_ai-monitor.log

[Install]
WantedBy=multi-user.target
"""
        
        # Create monitoring service script
        service_script = self.base_dir / "mcp_server" / "monitoring" / "service.py"
        service_script.parent.mkdir(exist_ok=True)
        
        with open(service_script, 'w') as f:
            f.write('''#!/usr/bin/env python3
"""Monitoring service for Cherry AI"""

import asyncio
from mcp_server.monitoring.performance import (
    get_performance_monitor,
    PerformanceOptimizer
)

async def main():
    monitor = get_performance_monitor()
    optimizer = PerformanceOptimizer(monitor)
    
    # Start monitoring and optimization tasks
    await asyncio.gather(
        monitor.collect_system_metrics(),
        optimizer.auto_optimize()
    )

if __name__ == "__main__":
    asyncio.run(main())
''')
        
        self.log("  ‚úì Monitoring service created")
    
    async def integrate_api(self):
        """Integrate all components with the API"""
        self.log("\nüîß Phase 4: Integrating API Components")
        
        # Update main API to include monitoring
        api_update = self.base_dir / "mcp_server" / "api" / "__init__.py"
        api_update.parent.mkdir(exist_ok=True)
        
        with open(api_update, 'w') as f:
            f.write('''"""Cherry AI API with single-user optimizations"""

from .main import app
from mcp_server.monitoring.performance import (
    PerformanceMiddleware,
    get_performance_monitor
)

# Add performance monitoring
app.add_middleware(PerformanceMiddleware)

# Start background monitoring
import asyncio

@app.on_event("startup")
async def startup_monitoring():
    monitor = get_performance_monitor()
    asyncio.create_task(monitor.collect_system_metrics())

__all__ = ["app"]
''')
        
        self.log("  ‚úì API integration completed")
    
    async def deploy_infrastructure(self):
        """Deploy the complete infrastructure"""
        self.log("\nüèóÔ∏è  Phase 5: Deploying Infrastructure")
        
        # Stop existing services
        self.log("  - Stopping existing services...")
        subprocess.run(
            ["docker-compose", "down", "-v"],
            cwd=self.base_dir,
            capture_output=True
        )
        
        # Create optimized docker-compose
        await self.create_optimized_docker_compose()
        
        # Start services using single-user compose file
        self.log("  - Starting optimized services...")
        result = subprocess.run(
            ["docker-compose", "-f", "docker-compose.single-user.yml", "up", "-d"],
            cwd=self.base_dir,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            raise Exception(f"Failed to start services: {result.stderr}")
        
        # Wait for services to be ready
        self.log("  - Waiting for services to be ready...")
        await asyncio.sleep(15)
        
        self.log("  ‚úì Infrastructure deployed")
    
    async def create_optimized_docker_compose(self):
        """Create optimized docker-compose for single user"""
        compose_override = {
            "version": "3.8",
            "services": {
                "api": {
                    "build": {
                        "context": ".",
                        "dockerfile": "Dockerfile.optimized"
                    },
                    "environment": [
                        "cherry_ai_CONTEXT=${cherry_ai_CONTEXT:-development}",
                        "cherry_ai_API_KEY=${cherry_ai_API_KEY}",
                        "PYTHONUNBUFFERED=1",
                        "OPTIMIZE_SINGLE_USER=true"
                    ],
                    "volumes": [
                        "./mcp_server:/app/mcp_server:ro",
                        "./config:/app/config:ro"
                    ],
                    "deploy": {
                        "resources": {
                            "limits": {
                                "cpus": "2.0",
                                "memory": "2G"
                            }
                        }
                    },
                    "healthcheck": {
                        "test": ["CMD", "curl", "-f", "http://localhost:8000/health"],
                        "interval": "30s",
                        "timeout": "10s",
                        "retries": 3
                    }
                },
                "postgres": {
                    "command": [
                        "postgres",
                        "-c", "shared_buffers=256MB",
                        "-c", "effective_cache_size=1GB",
                        "-c", "maintenance_work_mem=64MB",
                        "-c", "checkpoint_completion_target=0.9",
                        "-c", "wal_buffers=16MB",
                        "-c", "default_statistics_target=100",
                        "-c", "random_page_cost=1.1",
                        "-c", "effective_io_concurrency=200"
                    ]
                },
                "redis": {
                    "command": [
                        "redis-server",
                        "--maxmemory", "512mb",
                        "--maxmemory-policy", "allkeys-lru",
                        "--save", ""
                    ]
                }
            }
        }
        
        # Create optimized Dockerfile
        dockerfile_content = '''FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install additional performance packages
RUN pip install --no-cache-dir \
    uvloop \
    httptools \
    psutil \
    orjson

# Copy application code
COPY . .

# Optimize Python
ENV PYTHONOPTIMIZE=1
ENV PYTHONDONTWRITEBYTECODE=1

# Use uvloop for better async performance
ENV UVLOOP_USE_LIBUV=1

# Run with optimized settings
CMD ["uvicorn", "mcp_server.api.main:app", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "--loop", "uvloop", \
     "--workers", "1", \
     "--limit-concurrency", "100"]
'''
        
        with open(self.base_dir / "Dockerfile.optimized", 'w') as f:
            f.write(dockerfile_content)
        
        # Save override file
        import yaml
        with open(self.base_dir / "docker-compose.override.yml", 'w') as f:
            yaml.dump(compose_override, f, default_flow_style=False)
        
        self.log("  ‚úì Optimized Docker configuration created")
    
    async def run_comprehensive_tests(self):
        """Run comprehensive system tests"""
        self.log("\nüß™ Phase 6: Running Comprehensive Tests")
        
        tests = [
            ("Authentication", self.test_authentication),
            ("API Endpoints", self.test_api_endpoints),
            ("Performance", self.test_performance),
            ("Monitoring", self.test_monitoring),
            ("Database", self.test_database),
            ("Cache", self.test_cache)
        ]
        
        results = []
        for test_name, test_func in tests:
            try:
                await test_func()
                self.log(f"  ‚úì {test_name}: PASSED")
                results.append((test_name, True, None))
            except Exception as e:
                self.log(f"  ‚úó {test_name}: FAILED - {e}", "ERROR")
                results.append((test_name, False, str(e)))
        
        # Check if all tests passed
        if not all(result[1] for result in results):
            failed_tests = [r[0] for r in results if not r[1]]
            self.log(f"  ‚ö†Ô∏è  Failed tests: {', '.join(failed_tests)}", "WARNING")
    
    async def test_authentication(self):
        """Test authentication system"""
        import aiohttp
        
        api_key = os.getenv("cherry_ai_API_KEY", "")
        
        async with aiohttp.ClientSession() as session:
            # Test without auth
            async with session.get("http://localhost:8000/api/v1/info") as resp:
                assert resp.status == 401
            
            # Test with auth
            headers = {"X-API-Key": api_key}
            async with session.get("http://localhost:8000/api/v1/info", headers=headers) as resp:
                assert resp.status == 200
    
    async def test_api_endpoints(self):
        """Test API endpoints"""
        import aiohttp
        
        api_key = os.getenv("cherry_ai_API_KEY", "")
        headers = {"X-API-Key": api_key}
        
        async with aiohttp.ClientSession() as session:
            # Test health endpoint
            async with session.get("http://localhost:8000/health") as resp:
                assert resp.status == 200
                data = await resp.json()
                assert data["status"] == "healthy"
    
    async def test_performance(self):
        """Test performance metrics"""
        import aiohttp
        import statistics
        
        api_key = os.getenv("cherry_ai_API_KEY", "")
        headers = {"X-API-Key": api_key}
        
        response_times = []
        
        async with aiohttp.ClientSession() as session:
            # Make 10 requests and measure response time
            for _ in range(10):
                start = time.time()
                async with session.get("http://localhost:8000/api/v1/info", headers=headers) as resp:
                    assert resp.status == 200
                response_times.append((time.time() - start) * 1000)
        
        # Check average response time
        avg_response_time = statistics.mean(response_times)
        assert avg_response_time < 100, f"Average response time {avg_response_time}ms exceeds threshold"
    
    async def test_monitoring(self):
        """Test monitoring system"""
        import aiohttp
        
        api_key = os.getenv("cherry_ai_API_KEY", "")
        headers = {"X-API-Key": api_key}
        
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:8000/api/v1/metrics", headers=headers) as resp:
                assert resp.status == 200
                data = await resp.json()
                assert "metrics" in data
    
    async def test_database(self):
        """Test database connectivity"""
        # This would test actual database operations
        # For now, we'll just check if the service is running
        result = subprocess.run(
            ["docker-compose", "ps", "postgres"],
            cwd=self.base_dir,
            capture_output=True,
            text=True
        )
        assert "Up" in result.stdout
    
    async def test_cache(self):
        """Test cache system"""
        # Check Redis is running
        result = subprocess.run(
            ["docker-compose", "ps", "redis"],
            cwd=self.base_dir,
            capture_output=True,
            text=True
        )
        assert "Up" in result.stdout
    
    async def generate_deployment_report(self):
        """Generate comprehensive deployment report"""
        self.log("\nüìÑ Phase 7: Generating Deployment Report")
        
        report = {
            "deployment_id": f"deploy_{self.deployment_time.strftime('%Y%m%d_%H%M%S')}",
            "timestamp": self.deployment_time.isoformat(),
            "duration_seconds": (datetime.now() - self.deployment_time).total_seconds(),
            "context": os.getenv("cherry_ai_CONTEXT", "development"),
            "status": "SUCCESS",
            "components": {
                "authentication": {
                    "type": "single_user",
                    "context_based": True,
                    "api_key_configured": bool(os.getenv("cherry_ai_API_KEY"))
                },
                "monitoring": {
                    "enabled": True,
                    "metrics": ["system", "application", "custom"],
                    "auto_optimization": True
                },
                "infrastructure": {
                    "containers": ["api", "postgres", "redis", "weaviate"],
                    "optimizations": ["memory_limits", "cpu_limits", "health_checks"]
                }
            },
            "endpoints": {
                "api": "http://localhost:8000",
                "health": "http://localhost:8000/health",
                "metrics": "http://localhost:8000/api/v1/metrics"
            },
            "logs": self.deployment_log[-20:]  # Last 20 log entries
        }
        
        report_path = self.base_dir / f"deployment_report_{self.deployment_time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.log(f"  ‚úì Report saved to: {report_path}")
        
        # Display summary
        self.log("\n" + "=" * 60)
        self.log("üéâ DEPLOYMENT COMPLETE!")
        self.log("=" * 60)
        self.log(f"\nDeployment ID: {report['deployment_id']}")
        self.log(f"Duration: {report['duration_seconds']:.2f} seconds")
        self.log(f"Context: {report['context']}")
        self.log("\nAccess Points:")
        for name, url in report['endpoints'].items():
            self.log(f"  - {name}: {url}")
        
        self.log("\n‚úÖ Cherry AI is ready for use!")
    
    async def rollback(self):
        """Rollback deployment on failure"""
        self.log("\nüîÑ Rolling back deployment...", "WARNING")
        
        try:
            # Stop all services
            subprocess.run(
                ["docker-compose", "down"],
                cwd=self.base_dir,
                capture_output=True
            )
            
            # Remove generated files
            files_to_remove = [
                "docker-compose.override.yml",
                "Dockerfile.optimized",
                "config/monitoring.json"
            ]
            
            for file in files_to_remove:
                file_path = self.base_dir / file
                if file_path.exists():
                    file_path.unlink()
            
            self.log("  ‚úì Rollback completed")
        except Exception as e:
            self.log(f"  ‚úó Rollback failed: {e}", "ERROR")

if __name__ == "__main__":
    deployment = CompleteSystemDeployment()
    asyncio.run(deployment.run())