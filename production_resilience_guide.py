#!/usr/bin/env python3
"""
Production Resilience Best Practices Implementation Guide
For Cherry AI Orchestrator - Industry Standards & Patterns
"""

import os
import sys
import json
import subprocess
from datetime import datetime
from typing import Dict, List, Any

class ResilienceBestPractices:
    """Comprehensive guide to production resilience"""
    
    def __init__(self):
        self.server_ip = "150.136.94.139"
        self.username = "ubuntu"
        self.best_practices = {}
        
    def generate_complete_guide(self):
        """Generate comprehensive resilience implementation"""
        
        print("üèóÔ∏è PRODUCTION RESILIENCE BEST PRACTICES")
        print("=" * 60)
        print()
        
        # 1. Application Code Resilience
        self.application_resilience_patterns()
        
        # 2. Infrastructure Configuration
        self.infrastructure_resilience()
        
        # 3. Monitoring & Observability
        self.monitoring_observability()
        
        # 4. Deployment Strategies
        self.deployment_strategies()
        
        # 5. Security Hardening
        self.security_hardening()
        
        # Generate implementation scripts
        self.generate_implementation_scripts()
        
    def application_resilience_patterns(self):
        """Application-level resilience patterns"""
        
        patterns = {
            "1. Circuit Breaker Pattern": {
                "purpose": "Prevent cascading failures",
                "implementation": """
# Circuit Breaker Implementation
class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=60, expected_exception=Exception):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.expected_exception = expected_exception
        self._state = 'CLOSED'
        
    def call(self, func, *args, **kwargs):
        if self._state == 'OPEN':
            if self._timeout_expired():
                self._state = 'HALF_OPEN'
            else:
                raise Exception('Circuit breaker is OPEN')
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except self.expected_exception as e:
            self._on_failure()
            raise e
""",
                "benefits": ["Prevents system overload", "Fast failure detection", "Automatic recovery"]
            },
            
            "2. Retry with Exponential Backoff": {
                "purpose": "Handle transient failures gracefully",
                "implementation": """
# Retry Pattern
import asyncio
import random

async def retry_with_backoff(func, max_retries=3, base_delay=1, max_delay=60):
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            
            delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)
            await asyncio.sleep(delay)
""",
                "benefits": ["Handles temporary failures", "Reduces load during issues", "Configurable retry behavior"]
            },
            
            "3. Bulkhead Pattern": {
                "purpose": "Isolate resources to prevent total failure",
                "implementation": """
# Bulkhead Pattern using asyncio Semaphore
import asyncio

class BulkheadExecutor:
    def __init__(self, max_concurrent=10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
    async def execute(self, func, *args, **kwargs):
        async with self.semaphore:
            return await func(*args, **kwargs)
""",
                "benefits": ["Resource isolation", "Prevents resource exhaustion", "Maintains partial functionality"]
            },
            
            "4. Health Checks": {
                "purpose": "Enable automated recovery and load balancing",
                "implementation": """
# Comprehensive Health Checks
from enum import Enum

class HealthStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

class HealthChecker:
    async def check_database(self) -> Dict[str, Any]:
        try:
            # Check DB connection and query performance
            start = time.time()
            await db.execute("SELECT 1")
            latency = (time.time() - start) * 1000
            
            return {
                "status": HealthStatus.HEALTHY if latency < 100 else HealthStatus.DEGRADED,
                "latency_ms": latency
            }
        except Exception as e:
            return {"status": HealthStatus.UNHEALTHY, "error": str(e)}
    
    async def check_dependencies(self) -> Dict[str, Any]:
        checks = {
            "database": await self.check_database(),
            "cache": await self.check_cache(),
            "external_api": await self.check_external_api()
        }
        
        overall_status = HealthStatus.HEALTHY
        if any(c["status"] == HealthStatus.UNHEALTHY for c in checks.values()):
            overall_status = HealthStatus.UNHEALTHY
        elif any(c["status"] == HealthStatus.DEGRADED for c in checks.values()):
            overall_status = HealthStatus.DEGRADED
            
        return {"status": overall_status, "checks": checks}
""",
                "benefits": ["Automated recovery", "Load balancer integration", "Early problem detection"]
            },
            
            "5. Graceful Degradation": {
                "purpose": "Maintain core functionality during failures",
                "implementation": """
# Graceful Degradation Pattern
class DegradableService:
    def __init__(self):
        self.cache = {}
        self.fallback_mode = False
        
    async def get_data(self, key: str) -> Any:
        try:
            # Try primary data source
            if not self.fallback_mode:
                data = await self.fetch_from_primary(key)
                self.cache[key] = data
                return data
        except Exception as e:
            logger.warning(f"Primary source failed: {e}")
            self.fallback_mode = True
        
        # Fallback to cache
        if key in self.cache:
            logger.info("Serving from cache (degraded mode)")
            return self.cache[key]
        
        # Final fallback
        return self.get_default_response(key)
""",
                "benefits": ["Maintains availability", "Better user experience", "Automatic recovery"]
            }
        }
        
        self.best_practices["application_patterns"] = patterns
        
        print("üì± APPLICATION RESILIENCE PATTERNS")
        print("-" * 40)
        for pattern, details in patterns.items():
            print(f"\n{pattern}")
            print(f"Purpose: {details['purpose']}")
            print(f"Benefits: {', '.join(details['benefits'])}")
    
    def infrastructure_resilience(self):
        """Infrastructure configuration for resilience"""
        
        configs = {
            "1. Load Balancing": {
                "nginx_config": """
# Nginx Load Balancing with Health Checks
upstream backend {
    least_conn;  # or ip_hash for session persistence
    
    server backend1:8000 weight=5 max_fails=3 fail_timeout=30s;
    server backend2:8000 weight=5 max_fails=3 fail_timeout=30s;
    server backend3:8000 weight=1 backup;  # Backup server
    
    keepalive 32;  # Connection pooling
}

server {
    location / {
        proxy_pass http://backend;
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
        proxy_next_upstream_tries 3;
        proxy_connect_timeout 5s;
        proxy_read_timeout 60s;
        
        # Circuit breaker via nginx
        proxy_intercept_errors on;
        error_page 502 503 504 = @fallback;
    }
    
    location @fallback {
        return 503 '{"error": "Service temporarily unavailable"}';
    }
}
"""
            },
            
            "2. Auto-scaling": {
                "kubernetes_hpa": """
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: orchestra-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orchestra-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
"""
            },
            
            "3. Database Connection Pooling": {
                "config": """
# PostgreSQL Connection Pool Configuration
import asyncpg
from contextlib import asynccontextmanager

class DatabasePool:
    def __init__(self):
        self.pool = None
        
    async def init(self):
        self.pool = await asyncpg.create_pool(
            host='localhost',
            port=5432,
            user='orchestra',
            password='secure_password',
            database='orchestra_db',
            min_size=10,  # Minimum connections
            max_size=20,  # Maximum connections
            max_queries=50000,  # Queries before connection reset
            max_inactive_connection_lifetime=300,  # 5 minutes
            command_timeout=10,  # Query timeout
            statement_cache_size=20,  # Prepared statement cache
        )
    
    @asynccontextmanager
    async def acquire(self):
        async with self.pool.acquire() as connection:
            yield connection
"""
            },
            
            "4. Caching Strategy": {
                "redis_config": """
# Redis Caching with Fallback
import aioredis
import pickle
from typing import Optional, Any

class CacheManager:
    def __init__(self):
        self.redis = None
        self.local_cache = {}  # Fallback cache
        
    async def connect(self):
        self.redis = await aioredis.create_redis_pool(
            'redis://localhost:6379',
            minsize=5,
            maxsize=10,
            encoding='utf-8'
        )
    
    async def get(self, key: str) -> Optional[Any]:
        try:
            # Try Redis first
            value = await self.redis.get(key)
            if value:
                return pickle.loads(value)
        except Exception as e:
            logger.error(f"Redis error: {e}")
            
        # Fallback to local cache
        return self.local_cache.get(key)
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        try:
            # Set in Redis
            await self.redis.setex(key, ttl, pickle.dumps(value))
        except Exception as e:
            logger.error(f"Redis error: {e}")
        
        # Always update local cache
        self.local_cache[key] = value
"""
            }
        }
        
        self.best_practices["infrastructure"] = configs
        
        print("\n\nüèóÔ∏è INFRASTRUCTURE RESILIENCE")
        print("-" * 40)
        for config, details in configs.items():
            print(f"\n{config}")
    
    def monitoring_observability(self):
        """Monitoring and observability best practices"""
        
        monitoring = {
            "1. Structured Logging": {
                "implementation": """
# Structured Logging Setup
import structlog
import logging

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Usage
logger.info("api_request", 
    method="GET",
    path="/api/search",
    user_id="123",
    duration_ms=45,
    status_code=200
)
"""
            },
            
            "2. Metrics Collection": {
                "prometheus_metrics": """
# Prometheus Metrics
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

# Define metrics
request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration', ['method', 'endpoint'])
active_connections = Gauge('active_connections', 'Number of active connections')
error_rate = Counter('errors_total', 'Total errors', ['type'])

# Middleware to collect metrics
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    active_connections.inc()
    
    try:
        response = await call_next(request)
        request_count.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()
        return response
    except Exception as e:
        error_rate.labels(type=type(e).__name__).inc()
        raise
    finally:
        duration = time.time() - start_time
        request_duration.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(duration)
        active_connections.dec()

# Metrics endpoint
@app.get("/metrics")
async def metrics():
    return Response(content=generate_latest(), media_type="text/plain")
"""
            },
            
            "3. Distributed Tracing": {
                "opentelemetry_setup": """
# OpenTelemetry Tracing
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Setup tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Configure exporter
otlp_exporter = OTLPSpanExporter(endpoint="localhost:4317", insecure=True)
span_processor = BatchSpanProcessor(otlp_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Instrument FastAPI
FastAPIInstrumentor.instrument_app(app)

# Manual instrumentation
async def process_request(request_id: str):
    with tracer.start_as_current_span("process_request") as span:
        span.set_attribute("request.id", request_id)
        
        # Database operation
        with tracer.start_as_current_span("database_query"):
            result = await db.query("SELECT * FROM users")
            
        # External API call
        with tracer.start_as_current_span("external_api_call"):
            response = await external_api.call()
            
        return result
"""
            },
            
            "4. Alerting Rules": {
                "prometheus_alerts": """
# Prometheus Alerting Rules
groups:
  - name: orchestra_alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(errors_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"
      
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time"
          description: "95th percentile response time is {{ $value }} seconds"
      
      - alert: PodMemoryUsage
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Pod {{ $labels.pod }} memory usage is above 90%"
      
      - alert: DatabaseConnectionPoolExhausted
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "{{ $value }}% of connections are in use"
"""
            }
        }
        
        self.best_practices["monitoring"] = monitoring
        
        print("\n\nüìä MONITORING & OBSERVABILITY")
        print("-" * 40)
        for practice, details in monitoring.items():
            print(f"\n{practice}")
    
    def deployment_strategies(self):
        """Deployment best practices"""
        
        strategies = {
            "1. Blue-Green Deployment": {
                "description": "Zero-downtime deployment with instant rollback",
                "implementation": """
#!/bin/bash
# Blue-Green Deployment Script

BLUE_PORT=8000
GREEN_PORT=8001
CURRENT_ENV=$(cat /opt/orchestra/current_env)

if [ "$CURRENT_ENV" == "blue" ]; then
    NEW_ENV="green"
    NEW_PORT=$GREEN_PORT
else
    NEW_ENV="blue"
    NEW_PORT=$BLUE_PORT
fi

echo "Deploying to $NEW_ENV environment..."

# Deploy new version
docker run -d --name orchestra-$NEW_ENV -p $NEW_PORT:8000 orchestra:latest

# Health check
for i in {1..30}; do
    if curl -f http://localhost:$NEW_PORT/health; then
        echo "Health check passed"
        break
    fi
    sleep 2
done

# Switch traffic
sed -i "s/localhost:$CURRENT_PORT/localhost:$NEW_PORT/g" /etc/nginx/sites-enabled/orchestra
nginx -s reload

# Update current environment
echo $NEW_ENV > /opt/orchestra/current_env

# Stop old environment after 5 minutes
sleep 300
docker stop orchestra-$CURRENT_ENV
docker rm orchestra-$CURRENT_ENV
"""
            },
            
            "2. Canary Deployment": {
                "description": "Gradual rollout with automatic rollback",
                "kubernetes_config": """
# Flagger Canary Configuration
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: orchestra-api
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orchestra-api
  service:
    port: 8000
  analysis:
    interval: 1m
    threshold: 10
    maxWeight: 50
    stepWeight: 5
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      thresholdRange:
        max: 500
      interval: 1m
    webhooks:
    - name: load-test
      url: http://flagger-loadtester.test/
      metadata:
        cmd: "hey -z 1m -q 10 -c 2 http://orchestra-api-canary:8000/"
"""
            },
            
            "3. Feature Flags": {
                "description": "Control feature rollout without deployment",
                "implementation": """
# Feature Flag Implementation
from enum import Enum
from typing import Dict, Any
import json

class FeatureFlag(Enum):
    NEW_SEARCH_ALGORITHM = "new_search_algorithm"
    ENHANCED_CACHING = "enhanced_caching"
    BETA_UI = "beta_ui"

class FeatureFlagManager:
    def __init__(self):
        self.flags = self.load_flags()
        
    def load_flags(self) -> Dict[str, Any]:
        # Load from config file or database
        with open('/opt/orchestra/feature_flags.json', 'r') as f:
            return json.load(f)
    
    def is_enabled(self, flag: FeatureFlag, user_id: str = None) -> bool:
        flag_config = self.flags.get(flag.value, {})
        
        if not flag_config.get('enabled', False):
            return False
            
        # Check percentage rollout
        if 'percentage' in flag_config:
            import hashlib
            user_hash = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
            return (user_hash % 100) < flag_config['percentage']
            
        # Check user whitelist
        if 'users' in flag_config and user_id:
            return user_id in flag_config['users']
            
        return True

# Usage
feature_flags = FeatureFlagManager()

@app.get("/api/search")
async def search(query: str, user_id: str = None):
    if feature_flags.is_enabled(FeatureFlag.NEW_SEARCH_ALGORITHM, user_id):
        return await new_search_algorithm(query)
    else:
        return await legacy_search(query)
"""
            }
        }
        
        self.best_practices["deployment"] = strategies
        
        print("\n\nüöÄ DEPLOYMENT STRATEGIES")
        print("-" * 40)
        for strategy, details in strategies.items():
            print(f"\n{strategy}")
            print(f"Description: {details['description']}")
    
    def security_hardening(self):
        """Security best practices for production"""
        
        security = {
            "1. API Security": {
                "rate_limiting": """
# Advanced Rate Limiting
from typing import Dict, Tuple
import time
import hashlib

class RateLimiter:
    def __init__(self):
        self.requests: Dict[str, List[float]] = {}
        
    def is_allowed(self, identifier: str, limit: int = 100, window: int = 60) -> Tuple[bool, Dict]:
        now = time.time()
        key = hashlib.sha256(identifier.encode()).hexdigest()
        
        # Clean old requests
        if key in self.requests:
            self.requests[key] = [req_time for req_time in self.requests[key] 
                                 if now - req_time < window]
        else:
            self.requests[key] = []
        
        # Check limit
        if len(self.requests[key]) >= limit:
            return False, {
                "limit": limit,
                "remaining": 0,
                "reset": int(self.requests[key][0] + window)
            }
        
        # Add request
        self.requests[key].append(now)
        
        return True, {
            "limit": limit,
            "remaining": limit - len(self.requests[key]),
            "reset": int(now + window)
        }

# Middleware
rate_limiter = RateLimiter()

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    # Use IP + User ID for rate limiting
    identifier = f"{request.client.host}:{request.headers.get('user-id', 'anonymous')}"
    
    allowed, limits = rate_limiter.is_allowed(identifier)
    
    if not allowed:
        return JSONResponse(
            status_code=429,
            content={"error": "Rate limit exceeded"},
            headers={
                "X-RateLimit-Limit": str(limits['limit']),
                "X-RateLimit-Remaining": str(limits['remaining']),
                "X-RateLimit-Reset": str(limits['reset'])
            }
        )
    
    response = await call_next(request)
    
    # Add rate limit headers
    response.headers["X-RateLimit-Limit"] = str(limits['limit'])
    response.headers["X-RateLimit-Remaining"] = str(limits['remaining'])
    response.headers["X-RateLimit-Reset"] = str(limits['reset'])
    
    return response
""",
                "authentication": """
# JWT Authentication with Refresh Tokens
from datetime import datetime, timedelta
import jwt
from fastapi import HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

class AuthManager:
    def __init__(self, secret_key: str):
        self.secret_key = secret_key
        self.algorithm = "HS256"
        self.access_token_expire = timedelta(minutes=15)
        self.refresh_token_expire = timedelta(days=7)
        
    def create_access_token(self, user_id: str) -> str:
        payload = {
            "user_id": user_id,
            "type": "access",
            "exp": datetime.utcnow() + self.access_token_expire,
            "iat": datetime.utcnow()
        }
        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
    
    def create_refresh_token(self, user_id: str) -> str:
        payload = {
            "user_id": user_id,
            "type": "refresh",
            "exp": datetime.utcnow() + self.refresh_token_expire,
            "iat": datetime.utcnow()
        }
        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
    
    def verify_token(self, token: str, token_type: str = "access") -> Dict:
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            if payload.get("type") != token_type:
                raise HTTPException(status_code=401, detail="Invalid token type")
            return payload
        except jwt.ExpiredSignatureError:
            raise HTTPException(status_code=401, detail="Token expired")
        except jwt.InvalidTokenError:
            raise HTTPException(status_code=401, detail="Invalid token")

# Dependency
security = HTTPBearer()
auth_manager = AuthManager(secret_key=os.getenv("SECRET_KEY"))

async def get_current_user(credentials: HTTPAuthorizationCredentials = Security(security)):
    token = credentials.credentials
    payload = auth_manager.verify_token(token)
    return payload["user_id"]
"""
            },
            
            "2. Input Validation": {
                "pydantic_validation": """
# Comprehensive Input Validation
from pydantic import BaseModel, validator, constr, conint
from typing import Optional, List
import re

class SearchRequest(BaseModel):
    query: constr(min_length=1, max_length=200, strip_whitespace=True)
    mode: Optional[str] = "normal"
    limit: conint(ge=1, le=100) = 10
    offset: conint(ge=0) = 0
    
    @validator('query')
    def validate_query(cls, v):
        # Prevent SQL injection patterns
        sql_patterns = [
            r"(\b(SELECT|INSERT|UPDATE|DELETE|DROP|UNION|CREATE|ALTER)\b)",
            r"(--|#|/\*|\*/)",
            r"(\bOR\b.*=.*)",
            r"('|\"|;|\\)"
        ]
        
        for pattern in sql_patterns:
            if re.search(pattern, v, re.IGNORECASE):
                raise ValueError("Invalid characters in query")
        
        return v
    
    @validator('mode')
    def validate_mode(cls, v):
        allowed_modes = ['normal', 'advanced', 'regex']
        if v not in allowed_modes:
            raise ValueError(f"Mode must be one of {allowed_modes}")
        return v

# Usage
@app.post("/api/search")
async def search(request: SearchRequest):
    # Input is automatically validated
    return await perform_search(request.query, request.mode, request.limit, request.offset)
"""
            },
            
            "3. Security Headers": {
                "middleware": """
# Security Headers Middleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.middleware.cors import CORSMiddleware

# Trusted hosts
app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["*.orchestra.ai", "localhost"]
)

# CORS with specific origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://orchestra.ai", "https://app.orchestra.ai"],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["Authorization", "Content-Type"],
    max_age=86400  # 24 hours
)

# Security headers
@app.middleware("http")
async def security_headers(request: Request, call_next):
    response = await call_next(request)
    
    # Security headers
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    response.headers["Permissions-Policy"] = "geolocation=(), microphone=(), camera=()"
    
    return response
"""
            }
        }
        
        self.best_practices["security"] = security
        
        print("\n\nüîí SECURITY HARDENING")
        print("-" * 40)
        for practice, details in security.items():
            print(f"\n{practice}")
    
    def generate_implementation_scripts(self):
        """Generate ready-to-use implementation scripts"""
        
        print("\n\nüìù IMPLEMENTATION SCRIPTS")
        print("-" * 40)
        
        # 1. Complete setup script
        setup_script = """#!/bin/bash
# Complete Production Setup Script

set -e

echo "üöÄ Setting up production-grade Cherry AI Orchestrator"

# 1. Install dependencies
sudo apt-get update
sudo apt-get install -y python3-pip nginx postgresql redis-server prometheus grafana

# 2. Setup Python environment
cd /opt/orchestra
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 3. Configure PostgreSQL
sudo -u postgres psql << EOF
CREATE DATABASE orchestra_db;
CREATE USER orchestra_user WITH ENCRYPTED PASSWORD 'secure_password';
GRANT ALL PRIVILEGES ON DATABASE orchestra_db TO orchestra_user;
EOF

# 4. Configure Redis
sudo sed -i 's/