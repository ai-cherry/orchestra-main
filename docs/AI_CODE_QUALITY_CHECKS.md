# AI Code Quality Checks

This document provides an overview of the AI-assisted code quality checks implemented in the cherry_ai System. These checks are designed to address unique risks in AI-heavy codebases while maintaining development velocity by flagging potential issues for human review.

## Purpose

AI-heavy codebases introduce specific challenges that traditional code quality tools may not adequately address. The AI code quality checks focus on the following areas:

- **Hallucinated Code Patterns**: Detects code that may have been generated by AI tools and could contain inaccuracies or irrelevant content.
- **Prompt Injection Vectors**: Identifies potential vulnerabilities in LLM interactions where unsanitized user input could manipulate prompts.
- **RAG Context Window Management**: Validates that Retrieval-Augmented Generation (RAG) implementations properly manage context window limits to prevent issues like token overflow.
- **Model Version Drift**: Checks for outdated model versions or references in AI service integrations that could lead to compatibility or performance issues.

## Components

The AI code quality checks are implemented in the following components:

- **HallucinationDetector**: Uses regex patterns and heuristics to flag potential AI-generated code that may need review for correctness.
- **PromptInjectionChecker**: Analyzes prompt construction for unsanitized user input or unsafe concatenation practices.
- **RAGContextValidator**: Ensures context window management is implemented in RAG systems to handle token limits appropriately.
- **ModelVersionMonitor**: Scans for hardcoded model versions and flags outdated ones, suggesting updates to newer versions.

These components are housed in `packages/tools/src/ai_code_quality.py`.

## Pre-Commit Hook

A pre-commit hook script is provided to run these checks automatically on staged Python files before a commit is made. The script is located at `scripts/pre-commit-ai-checks.py`.

### Installation

To install the pre-commit hook, follow these steps:

1. Ensure the script is executable:
   ```bash
   chmod +x scripts/pre-commit-ai-checks.py
   ```
2. Link the script to the Git hooks directory:
   ```bash
   ln -sf ../../scripts/pre-commit-ai-checks.py .git/hooks/pre-commit
   ```

Alternatively, you can create a setup script to automate this process.

### Usage

Once installed, the pre-commit hook will run automatically when you attempt to commit changes. It will check staged Python files for AI code quality issues. If issues are found, it will display warnings with line numbers and descriptions, along with suggestions for review. The commit will not be blocked, allowing you to proceed if necessary, but it encourages addressing the flagged issues.

Example output:

```
Running AI Code Quality Checks on staged files...

AI Code Quality Issues in packages/llm/src/models/openai.py:
  Line 46: Outdated model 'gpt-3.5-turbo' detected. Consider updating to gpt-4 or gpt-4o
    effective_model = settings.DEFAULT_LLM_MODEL_FALLBACK_OPENAI or "gpt-3.5-turbo"
  Suggestion: Review these issues for correctness and security before committing.

WARNING: AI Code Quality issues were found in staged files.
You can commit anyway, but it's recommended to review the flagged code.
```

## Running Checks Manually

You can run the AI code quality checks manually on a specific directory or file by invoking the `run_ai_code_quality_checks` function from `ai_code_quality.py`. For example:

```bash
python -m packages.tools.src.ai_code_quality .
```

This will check all Python files in the current directory and its subdirectories, reporting any issues found.

## Extending the Checks

The AI code quality checks are designed to be extensible. You can add new patterns or rules to the existing detectors or create new detector classes in `ai_code_quality.py`. To add a new check:

1. Define a new class with a descriptive name (e.g., `NewIssueDetector`).
2. Implement the `check_file` method to return a list of issues found in a file.
3. Add the new detector to the list in `run_ai_code_quality_checks` function.

This allows the system to grow with the project's needs, incorporating additional checks for emerging AI-related code quality concerns.

## Interpreting Output

The output from the checks includes:

- **File Path**: The file where issues were detected.
- **Line Number**: The specific line of code with the potential issue.
- **Issue Description**: A brief explanation of the detected issue.
- **Line Content**: The actual code line for quick reference.
- **Suggestion**: A recommendation for addressing the issue, typically involving human review for correctness or security.

By focusing on these areas, the AI code quality checks help maintain the integrity and security of the codebase while supporting rapid development in an AI-driven environment.
