# Comprehensive AI Codebase Optimization, Cleanup, and Automation Framework

## Executive Summary

This framework establishes enterprise-grade standards for AI-assisted development within Project Symphony. It combines intelligent cleanup protocols with performance-first coding principles, addresses the proliferation of temporary files and one-off scripts, and ensures that legitimate automation scripts are properly scheduled, managed, and maintained. The goal is to create a clean, performant, efficient, and well-organized codebase, while guiding AI assistants to be better citizens of our development ecosystem.

## Part 1: Core Principles for AI-Assisted Development

These principles must be ingrained in all AI-assisted development workflows and configurations.

### 1. Performance-First Mindset
*   **Benchmarking:** All critical AI-generated functions must include benchmarks.
*   **Algorithmic Efficiency:** Prefer algorithms with O(n log n) or better time complexity unless a simpler, less complex solution is demonstrably sufficient for the specific scale and context. Provide justification if deviating.
*   **Resource Monitoring:** Implement or integrate with existing mechanisms for monitoring memory and CPU usage of AI-generated components.
*   **Database Queries (PostgreSQL Focus):** All generated database queries must include `EXPLAIN ANALYZE` plans during development and testing to ensure efficiency. Optimize queries based on these plans.
*   **Network Calls:** Implement sensible timeouts, retries (with backoff), and consider circuit breaker patterns for AI-generated network-dependent code.

### 2. Zero Junk Policy
*   **Temporary File Lifecycle:**
    *   Temporary files generated by AI processes must have a clearly defined, short lifecycle (e.g., self-destruct or be registered for cleanup within 24-72 hours).
    *   Utilize the `transient_file` decorator or similar mechanisms for Python scripts.
*   **Artifact Documentation:** All significant AI-generated artifacts (models, datasets, complex configuration files) must have documented ownership, purpose, and an expiration/review date.
*   **No Standalone Scripts (Default):**
    *   Avoid generating one-off executable scripts for cleanup, data migration, or utility tasks.
    *   **Prefer Integration:** Functionality should be integrated into existing, relevant modules (e.g., `utils.py`, `data_services.py`, `scripts/cherry_ai.py` CLI commands) as reusable functions, classes, or CLI subcommands.
    *   **Exception:** Standalone scripts are permissible only if part of a defined, temporary workflow (e.g., a single-use database migration script that self-documents its completion and obsolescence, and is removed post-execution). Such scripts must be explicitly approved and tracked.
*   **Read-Only Outputs:**
    *   **Logs:** Configure application logging to use the central logging system with appropriate log levels and rotation. Avoid AI generating ad-hoc log files.
    *   **Reports/Documentation:** Generate reports or documentation directly into version-controlled final locations (e.g., `docs/final_report.md`) or a well-structured draft area that is part of a review workflow. Encourage updating existing documents. Avoid littering the repository with numerous `draft_v[n].md` files.
    *   **Intermediate Data:** Intermediate data processing steps should ideally be handled in-memory or via temporary files that are explicitly cleaned up by the AI-generated process itself within the same execution. If intermediate data must be saved, it must be in a designated `temp/` directory (which is regularly purged or `.gitignore`\'d) and follow the `transient_file` lifecycle.

### 3. Context-Aware Generation
*   **Project Context Registry:** AI tools should leverage and contribute to a project-wide context registry (e.g., understanding existing modules, services, database schemas via `shared.database.UnifiedDatabase`) for cross-component optimization and to avoid redundant implementations.
*   **Interface Contracts:** Enforce strict and clear interface contracts between AI-generated modules and existing project components.
*   **Dependency Declaration:** All AI-generated code modules or significant files must declare their purpose, dependencies, and ideally, their expected lifecycle or review cadence (e.g., in a header comment or accompanying metadata file).

### 4. AI Interaction Guidelines
*   **Prompt for Integration:** When asked to perform a task that could result in a script or standalone data file, the AI should proactively ask: "Would you like me to integrate this logic into an existing module (e.g., `[relevant_module.py]`), or generate a new [report/document] in the standard location (e.g., `docs/` or `reports/`)? If so, which module/location?"
*   **Refactoring Suggestions:** If the AI observes patterns suggesting multiple requests for one-off scripts or similar temporary files, it should suggest refactoring the common logic into a reusable component or standardizing the output format/location.
*   **"Act As If" Persona:** AIs should operate with the persona of a "Senior Python Developer focused on long-term code hygiene, performance, simplicity, and minimizing repository clutter within Project Symphony."
*   **Clarity of Purpose and Lifecycle for any generated file:**
    *   If a standalone file is unavoidably necessary (and approved), the AI must:
        *   Prompt for/use a clear naming convention indicating purpose and lifecycle (e.g., `temp_user_migration_2024-07-15_to_be_deleted_after_run.py`, `interim_report_q2_review_draft_expires_2024-08-01.md`).
        *   Include comments or metadata within the file detailing its temporary nature and removal criteria/timeline.
        *   Suggest adding it to `.gitignore` if purely local and temporary.

## Part 2: Tool-Specific Configuration & Guidance Updates

These configurations and guidelines should be implemented in the respective AI assistant tools and project rule files.

### 1. Roo AI System

**File: `.roo/config/performance_rules.json` (or equivalent master config)**
```json
{
  "globalSettings": {
    "performanceFirst": true,
    "zeroJunkPolicy": true,
    "autoCleanupIntegration": true, // Indicates awareness of cleanup scripts
    "defaultPythonVersion": "3.10",
    "primaryDataStore": "PostgreSQL",
    "vectorStore": "Weaviate",
    "databaseAccessClass": "shared.database.UnifiedDatabase"
  },
  "modePriorities": {
    "architect": {
      "model": "anthropic/claude-3.5-sonnet", // Or latest powerful model
      "temperature": 0.2,
      "constraints": [
        "no-temp-files-without-lifecycle",
        "integration-preferred-over-standalone",
        "performance-analysis-mandatory-for-db-io",
        "adhere-to-project-structure",
        "use-pulumi-python-Lambda-for-iac"
      ],
      "optimizationRules": {
        "dependency-minimization": true,
        "cache-strategy-awareness": "application-level", // Be aware of existing caching
        "resource-budgeting": "contextual" // Consider typical resource limits
      },
      "instructions": [
        "Prioritize integrating new functionalities into existing modules.",
        "Ensure all PostgreSQL queries are optimized and include EXPLAIN ANALYZE in development.",
        "Design for simplicity and maintainability using established project patterns."
      ]
    },
    "code": {
      "model": "anthropic/claude-3.5-sonnet", // Or latest powerful model
      "temperature": 0.1,
      "constraints": [
        "max-file-size: 700kb", // Adjusted for practical modules
        "cyclomatic-complexity < 15",
        "no-standalone-scripts-by-default",
        "benchmark-required-for-loops-over-1k-iterations",
        "type-hints-mandatory",
        "google-style-docstrings"
      ],
      "instructions": [
        "Generate Python 3.10 compatible code.",
        "Always use type hints and Google-style docstrings.",
        "If creating a temporary file, use the \'transient_file\' decorator or ensure explicit cleanup.",
        "Avoid \'os.system()\'; use \'subprocess.run()\'.",
        "Access databases ONLY via \'shared.database.UnifiedDatabase\'."
      ]
    },
    "debug": {
      "model": "anthropic/claude-3.5-sonnet",
      "temperature": 0.05,
      "constraints": [
        "no-debug-print-to-stdout-in-prod-code",
        "integrate-logging-via-project-logger",
        "performance-profiling-for-bottlenecks"
      ],
      "instructions": [
        "Identify root causes and suggest fixes adhering to project standards.",
        "Do not add temporary debug files; use integrated logging."
      ]
    }
  },
  "cleanupProtocols": {
    "awareness": {
      "inventoryScript": "scripts/comprehensive_inventory.sh",
      "cleanupEngine": "scripts/cleanup_engine.py",
      "transientFileDecorator": "core.utils.file_management.transient_file" // Example path
    },
    "autoPurgeSuggestionPatterns": ["tmp_*", "temp_*", "ai_generated_debug_*", "test_output_*", "draft_v*_*.md", "*.bak", "*.tmp"]
  },
  "projectSpecificRules": [
    ".roo/rules-architect/01-core-principles.md",
    ".roo/rules-code/01-coding-standards.md"
  ]
}
```

**File: `.roo/rules-architect/01-core-principles.md**
```markdown
## Architecture Principles for Roo AI (Project Symphony)
- **Integration First:** Prioritize extending existing modules (`scripts/`, `core/`, `services/`, `shared/`) over creating new standalone components or scripts.
- **Database:** Use only PostgreSQL (via `shared.database.UnifiedDatabase`) for relational data and Weaviate for vector data.
- **Performance:** Design for performance. All database operations must be analyzed (`EXPLAIN ANALYZE`). CPU/memory intensive operations should be flagged.
- **Simplicity:** Solutions should be simple, stable, and maintainable, leveraging existing project patterns.
- **IaC:** Infrastructure as Code must use Pulumi with Python for lambda.
- **File Management:**
    - Strictly no temporary files without explicit, automated lifecycle management (e.g., `transient_file` decorator, registration with cleanup service).
    - All significant generated files must declare purpose and an expiration/review date.
    - Prefer in-memory processing for intermediate data over temporary file-based workflows.
- **Configuration:** Use `config/` directory for configurations. Validate with `scripts/config_validator.py`.
- **Automation:** New automation tasks should be integrated into `scripts/cherry_ai.py` or existing scheduled jobs, not as new standalone cron entries without registration.
```

**File: `.roo/rules-code/01-coding-standards.md**
```markdown
## Code Generation Standards for Roo AI (Project Symphony)
- **Python Version:** Python 3.10. No Python 3.11+ specific features (e.g., `match/case`, `tomllib`).
- **Dependencies:** Use `pip/venv` only. Check `requirements/base.txt`. Avoid adding heavy dependencies for simple tasks.
- **Formatting:** Adhere to Black. Use isort for imports.
- **Type Hints:** Mandatory for all function/method signatures and important variable declarations (PEP 484).
- **Docstrings:** Google-style for all public modules, classes, and functions.
- **Error Handling:** Use specific exceptions. Implement `try-except` blocks appropriately.
- **Subprocesses:** Use `subprocess.run()` instead of `os.system()`.
- **Database:** ONLY use `shared.database.UnifiedDatabase`. No direct DB connections.
- **Anti-Patterns to Avoid:**
    - âŒ Creating standalone cleanup scripts (integrate into `cleanup_engine.py` or `scripts/cherry_ai.py`).
    - âŒ Generating temporary data files without lifecycle management.
    - âŒ Writing one-off utility scripts (integrate into existing `utils` or service modules).
    - âŒ Creating debug output files (use project\'s logging framework).
- **Required Patterns:**
    - âœ… Integrate utility functions into existing modules (e.g., `core.utils`, `shared.utils`).
    - âœ… Use the application\'s central logging system instead of print statements or ad-hoc file outputs.
    - âœ… Implement cleanup logic within the primary function/process or use `transient_file` for outputs.
    - âœ… Add performance considerations (benchmarks, complexity analysis) to new complex code.
```

### 2. Cursor AI Assistant

**File: `.cursorrules` (or equivalent ` lÃ nguage_settings.json` / `custom_prompts.md`)**
```markdown
## Cursor AI Rules for Project Symphony

### General Guidance
- **Act as a Senior Python Developer for Project Symphony.** Focus on performance, stability, simplicity, maintainability, and code hygiene.
- **Python 3.10 Only:** Ensure all generated Python code is compatible with Python 3.10.
- **Existing Patterns:** Prioritize using existing project patterns and modules. Before creating new code, check if similar functionality exists in `scripts/`, `core/`, `shared/`, or `services/`.
- **Database Access:** Strictly use `shared.database.UnifiedDatabase` for all PostgreSQL and Weaviate interactions.
- **No New Heavy Dependencies:** Justify any new external library by significant simplification or performance gains, and ensure it\'s added to `requirements/base.txt`.
- **No Docker/Poetry:** Do not generate Dockerfiles, docker-compose files, or use Poetry/Pipfile. This project uses `pip/venv`.

### Performance & Optimization Guardrails
- **Database Queries:** For any new or modified PostgreSQL query, provide the `EXPLAIN ANALYZE` plan.
- **Complex Operations:** For functions with high computational complexity or significant memory usage, flag them for review and suggest potential optimizations or resource pooling strategies if applicable.
- **Network Calls:** Implement robust error handling, timeouts, and retries for network calls.

### File Lifecycle Management & Anti-Junk Policy
- **No Temporary Files Without Lifecycle:** Do not generate temporary files unless they are managed by a decorator like `transient_file` or explicitly cleaned up within the same operational scope.
- **Register Generated Files:** If a file needs to persist beyond immediate execution, it should ideally be registered with the cleanup system (`.cleanup_registry.json`) or have a clear expiration mechanism.
- **Prefer Streaming/In-Memory:** For data processing, prefer streaming or in-memory operations over intermediate file-based steps.
- **Log Management:** All logging output must go through the project\'s established logging framework. Do not create separate log files. Rotating handlers are configured centrally.
- **Output Destination:** Ask for clarification if an output destination is unclear. Default to integrating into existing structures or standard output locations (`docs/`, `reports/`). Avoid littering the root or module directories with loose output files.

### Integration Requirements
- **Module Extension:** New functionality should extend existing modules where appropriate.
- **Utility Functions:** Place general utility functions in a relevant `utils.py` file within an existing appropriate directory structure (e.g., `core/utils/`, `shared/utils/`).
- **Database Operations:** Leverage existing database connection pools and helper functions within `shared.database.UnifiedDatabase`.
- **Error Handling:** Integrate error handling with the main logging system and use project-defined specific exceptions.

### Automation Scripts
- **Scheduling:** If a script requires scheduling, state this clearly. The `AutomationManager` (`scripts/automation_manager.py`) and standard system tools (`cron`, `systemd`) are used for this. Do not implement custom scheduling logic.
- **Idempotency:** Ensure any script designed for repeated execution is idempotent.
- **Health & Logging:** Automated scripts must include health check mechanisms and log their execution status to the centralized system.
```

### 3. Factory AI Droid Directives

**File: `factory_ai_config.yaml` (conceptual, for configuring hypothetical "Droids")**
```yaml
# factory_ai_config.yaml
# Directives for Factory AI Droids (Code Generation & Management Bots)

# General Principles for All Droids
global_directives:
  - "Adhere to Project Symphony Core Principles: Performance-First, Zero Junk, Context-Awareness."
  - "Python 3.10, pip/venv, Black formatting, Google Docstrings, Type Hints are mandatory."
  - "Use \'shared.database.UnifiedDatabase\' for all database interactions (PostgreSQL, Weaviate)."
  - "Prioritize integration into existing modules over creating standalone scripts/files."
  - "All generated files with a lifecycle beyond immediate execution MUST use \'transient_file\' or be registered for cleanup."

# Specific Droid Configurations
code_generation_droid_v3:
  name: "SymphonyCodeGenDroid"
  version: "3.1.0"
  description: "Generates production-quality Python code for Project Symphony."
  operational_parameters:
    resource_budgeting:
      # Droid itself, not the code it generates, but informs its complexity choices
      cpu_max_self: "50%" 
      memory_cap_self: "4GB"
      # Guidance for generated code:
      target_code_disk_temp_allowance: "0MB (unless explicitly managed by transient_file or similar)"
    
    optimization_matrix_for_generated_code: # Targets for the code it produces
      latency_target_ms: "<250ms (P99 for critical paths)"
      throughput_min_rps: ">500 (for high-load services)"
      error_rate_max_percent: "<0.05%"
    
    clean_coding_protocols_for_generated_code:
      # sonarqube_compliance_target: "A-Rating" # If applicable
      technical_debt_ratio_target_percent: "<5"
      performance_benchmarks_generation: "required_for_complex_logic"
      security_scan_integration: "required_before_commit_suggestion" # e.g. bandit

  file_management_directives:
    temp_file_policy: "forbidden_without_explicit_lifecycle_decorator"
    integration_preference_score: 0.9 # High preference for integrating into existing code
    cleanup_automation_awareness: true # Aware of cleanup_engine.py
    default_output_suggestion: "integrate_or_docs_folder"

  automation_script_generation_rules:
    scheduling_requirement_query: true # "Does this script need to run automatically?"
    health_monitoring_integration: "suggest_standard_pattern"
    failure_notification_stub: "suggest_logging_critical_error"
    idempotency_check: "analyze_and_warn_if_not_idempotent"
    target_for_new_automation: "scripts/cherry_ai.py CLI or registered via AutomationManager"

# Other droids (e.g., RefactoringDroid, TestingDroid) would have similar tailored directives.
```

## Part 3: Comprehensive Cleanup Implementation Plan

This plan outlines a multi-phase approach to identify, remove, and manage obsolete files and scripts.

### Phase 1: Intelligent Inventory System

**Script: `scripts/comprehensive_inventory.sh`**
(This script is designed to be run from the project root.)
```bash
#!/bin/bash
# comprehensive_inventory.sh - Smart file discovery and analysis for Project Symphony

set -euo pipefail # Exit on error, undefined variable, or pipe failure

INVENTORY_FILE="cleanup_inventory.json"
PROJECT_ROOT="${1:-.}" # Allow specifying project root, default to current dir

echo "ðŸ” Starting comprehensive codebase inventory in \'$PROJECT_ROOT\'..."

# Ensure project root is an absolute path
PROJECT_ROOT_ABS="$(cd "$PROJECT_ROOT" && pwd)"

# Function to analyze file purpose and lifecycle
analyze_file() {
    local file_rel_path="$1" # Relative path from where find starts
    local base_dir="$2"      # Base directory find was run from
    local file_abs_path="$base_dir/$file_rel_path"

    # Normalize path by removing leading ./
    file_abs_path="${file_abs_path/.\//}"
    
    local file_type=""
    local purpose_heuristic="unknown"
    local expiration_info="none"
    local references_count=0
    local git_tracked_status="false"
    local owner_info="unknown"

    # Determine file type (basic)
    if [[ -x "$file_abs_path" ]]; then
        file_type="executable_generic"
        if [[ "$file_abs_path" == *.sh ]]; then file_type="shell_script"; 
        elif [[ "$file_abs_path" == *.py ]]; then file_type="python_script"; fi
    elif [[ "$file_abs_path" == *.py ]]; then file_type="python_module";
    elif [[ "$file_abs_path" == *.js ]]; then file_type="javascript";
    elif [[ "$file_abs_path" == *.sh ]]; then file_type="shell_script_non_exec";
    elif [[ "$file_abs_path" =~ \.(txt|log|md|csv|json|xml|html|yaml|yml|rst)$ ]]; then file_type="text_data_doc";
    elif [[ "$file_abs_path" =~ \.(docx|pdf|odt)$ ]]; then file_type="binary_doc";
    else
        file_type="other_binary_or_unknown"
    fi

    # Check for references (simple grep, might be slow for large repos, consider `git grep`)
    # Search for filename without path to catch more references
    local filename_only
    filename_only=$(basename "$file_abs_path")
    references_count=$(git grep -lw "$filename_only" -- "$PROJECT_ROOT_ABS" | wc -l)
    
    # Extract metadata if present (example: "Expires: YYYY-MM-DD")
    if head -n 5 "$file_abs_path" | grep -q -i "expires:"; then
        expiration_info=$(head -n 5 "$file_abs_path" | grep -i "expires:" | head -1 | sed \'s/.*[Ee]xpires: //\')
    fi

    # Check Git status
    if git -C "$PROJECT_ROOT_ABS" ls-files --error-unmatch "$file_abs_path" >/dev/null 2>&1; then
        git_tracked_status="true"
        # Get last committer (can be slow per file, consider batching or alternative approach if too slow)
        # owner_info=$(git -C "$PROJECT_ROOT_ABS" log -1 --pretty=format:\'%an\' -- "$file_abs_path")
    fi
    
    # File modification timestamp (Unix epoch)
    local modified_timestamp
    if [[ "$(uname)" == "Darwin" ]]; then
        modified_timestamp=$(stat -f%m "$file_abs_path")
    else
        modified_timestamp=$(stat -c%Y "$file_abs_path")
    fi

    # File size
    local file_size
    if [[ "$(uname)" == "Darwin" ]]; then
        file_size=$(stat -f%z "$file_abs_path")
    else
        file_size=$(stat -c%s "$file_abs_path")
    fi

    # Heuristic for purpose based on name/path
    if [[ "$file_rel_path" =~ (tmp/|temp/|\.tmp$|\.bak$|_backup[0-9-]*) ]]; then
        purpose_heuristic="temporary_or_backup"
    elif [[ "$file_rel_path" =~ (log/|logs/|\.log$) ]]; then
        purpose_heuristic="log_file"
    elif [[ "$file_rel_path" =~ (draft|report|output|ai_generated_|_to_be_deleted_) ]]; then
        purpose_heuristic="generated_output_or_draft"
    elif [[ "$file_rel_path" =~ (scripts/|tools/) && ( "$file_type" == "python_script" || "$file_type" == "shell_script" ) ]]; then
         purpose_heuristic="utility_script"
    fi

    # Create JSON object per file
    jq -n --arg path "$file_abs_path" \
          --arg type "$file_type" \
          --argjson size "$file_size" \
          --arg modified_epoch "$modified_timestamp" \
          --argjson references "$references_count" \
          --arg expiration "$expiration_info" \
          --argjson git_tracked "$git_tracked_status" \
          --arg owner "$owner_info" \
          --arg purpose "$purpose_heuristic" \
          \'{path: $path, type: $type, size: $size, modified_epoch: $modified_epoch, references: $references, expiration: $expiration, git_tracked: $git_tracked, owner: $owner, purpose_heuristic: $purpose}\'
}

export -f analyze_file # Export function for find -exec
export PROJECT_ROOT_ABS # Export for analyze_file

# Define exclusion patterns for find
# IMPORTANT: Customize these exclusions for your project
EXCLUDE_PATTERNS=(
    -path "*/.git/*" \
    -o -path "*/.venv/*" \
    -o -path "*/venv/*" \
    -o -path "*/node_modules/*" \
    -o -path "*/bower_components/*" \
    -o -path "*/dist/*" \
    -o -path "*/build/*" \
    -o -path "*/target/*" \
    -o -path "*/__pycache__/*" \
    -o -path "*/.pytest_cache/*" \
    -o -path "*/.mypy_cache/*" \
    -o -path "*/.hypothesis/*" \
    -o -path "*/docs/official_releases/*"  # Example critical doc dir
    -o -path "*/data/production_seeds/*" # Example critical data dir
    -o -path "*/requirements/frozen/*" # Frozen requirements
    # Add more project-specific exclusions here
)

# Scan for all files initially, then filter in jq or Python for more complex logic
# This find command lists all files and pipes to jq for aggregation.
# analyze_file is called for each file.
find "$PROJECT_ROOT_ABS" \( "${EXCLUDE_PATTERNS[@]}" \) -prune -o -type f -print0 | \
    xargs -0 -I {} bash -c \'analyze_file "$@"\' _ {} "$PROJECT_ROOT_ABS" | \
    jq -s \'.\' > "$PROJECT_ROOT_ABS/$INVENTORY_FILE"

INVENTORY_COUNT=$(jq \'length\' "$PROJECT_ROOT_ABS/$INVENTORY_FILE")
echo "ðŸ“Š Inventory complete. Scanned $INVENTORY_COUNT files. Report: $PROJECT_ROOT_ABS/$INVENTORY_FILE"
echo "â„¹ï¸  Note: \'references\' count is a simple grep. \'owner\' info might be basic."
echo "Next step: Run \'python scripts/cleanup_engine.py $PROJECT_ROOT_ABS/$INVENTORY_FILE\'"
```
**Key features of `comprehensive_inventory.sh`:**
*   Takes an optional argument for the project root.
*   Generates a `cleanup_inventory.json` file.
*   Extracts path, type, size, modification date, simple reference count, expiration (if found in first 5 lines), and Git tracking status.
*   Applies a basic heuristic for file purpose.
*   Provides a customizable list of directory/path exclusions.

### Phase 2: Context-Aware Cleanup Engine

**Script: `scripts/cleanup_engine.py`**
```python
#!/usr/bin/env python3
# cleanup_engine.py - Intelligent file cleanup with safety checks for Project Symphony

import json
import os
import subprocess
import logging
from pathlib import Path
from datetime import datetime, timedelta, timezone
import sys
import re

# --- Configuration ---
# Critical directories (relative to project root) that should largely be ignored
CRITICAL_DIRS = {
    ".git", "venv", ".venv", "node_modules", "dist", "build", 
    "requirements/frozen", "docs/official_releases", "data/production_seeds",
    # Add core application directories if they should not contain auto-deletable files
    "core", "shared", "services", "scripts" # These are less about deletion, more about not flagging their contents as "junk" by default
}
# Patterns for files/directories that should always be protected (basename matching)
PROTECTED_PATTERNS = [
    re.compile(r"deploy_.*\.sh"),
    re.compile(r"migration_.*\.py"),
    re.compile(r"Pulumi\.(yaml|.*\.yaml)"),
    re.compile(r"README\.md", re.IGNORECASE),
    re.compile(r"\.gitignore"),
    re.compile(r"pyproject\.toml"),
    re.compile(r"requirements.*\.txt"),
    re.compile(r"Makefile"),
    # Add project critical file patterns
]
# File extensions often associated with temporary/generated files
POTENTIALLY_OBSOLETE_EXTENSIONS = {
    ".tmp", ".bak", ".log", ".output", ".err", ".swp", ".swo", 
    ".DS_Store", "*.pyc" 
}
# Name patterns often associated with temporary/generated files
POTENTIALLY_OBSOLETE_NAME_PATTERNS = [
    re.compile(r"^tmp_.*"),
    re.compile(r"^temp_.*"),
    re.compile(r".*_temp$"),
    re.compile(r"^ai_generated_.*"),
    re.compile(r"^debug_.*"),
    re.compile(r"^test_output_.*"),
    re.compile(r"^draft_v[0-9]+_.*"),
    re.compile(r".*_backup_[0-9]{8}.*"), # e.g. file_backup_20230101.zip
]

LOG_FILE = Path("cleanup_actions.log")
CLEANUP_REGISTRY_FILE = Path(".cleanup_registry.json") # For explicit lifecycle management

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler(sys.stdout)
    ]
)

class IntelligentCleanup:
    def __init__(self, inventory_file_path: str, project_root_path: str, dry_run: bool = True):
        self.project_root = Path(project_root_path).resolve()
        self.inventory_path = Path(inventory_file_path).resolve()
        self.inventory: list[dict] = self._load_json(self.inventory_path)
        self.cleanup_registry: dict = self._load_json(self.project_root / CLEANUP_REGISTRY_FILE, default={})
        self.dry_run = dry_run
        self.automation_scripts: set[str] = self._find_scheduled_scripts()

        if not self.inventory:
            logging.warning(f"Inventory file {inventory_file_path} is empty or not found.")
            sys.exit(1)

    def _load_json(self, file_path: Path, default=None) -> any:
        try:
            with file_path.open(\'r\') as f:
                return json.load(f)
        except FileNotFoundError:
            if default is not None:
                return default
            logging.error(f"File not found: {file_path}")
            sys.exit(1)
        except json.JSONDecodeError:
            logging.error(f"Invalid JSON in file: {file_path}")
            if default is not None:
                return default
            sys.exit(1)

    def _find_scheduled_scripts(self) -> set[str]:
        """Rudimentary check for scheduled scripts (crontab for current user). More robust checks needed for system-wide."""
        scheduled = set()
        try:
            cron_output = subprocess.check_output([\'crontab\', \'-l\'], stderr=subprocess.DEVNULL, text=True)
            # This is a very basic parser, real cron parsing is complex
            for line in cron_output.splitlines():
                if line.strip().startswith(\'#\') or not line.strip():
                    continue
                parts = line.split()
                if len(parts) > 5:
                    command_part = " ".join(parts[5:])
                    # Try to find scripts within the project root
                    # This requires script paths in cron to be absolute or identifiable
                    for item in re.split(r\'\s+|;|&', command_part):
                        if item.startswith(str(self.project_root)) and Path(item).is_file():
                            scheduled.add(str(Path(item).resolve()))
        except (subprocess.CalledProcessError, FileNotFoundError):
            logging.warning("Crontab not found or no entries for current user. Scheduled script check may be incomplete.")
        # Add systemd timer checks if relevant for your environment
        return scheduled

    def is_safe_to_remove(self, file_info: dict) -> tuple[bool, str]:
        """Determine if file is safe to remove based on multiple criteria. Returns (is_safe, reason)."""
        file_path = Path(file_info[\'path\']).resolve()
        relative_path_str = str(file_path.relative_to(self.project_root))

        # 1. Critical Directory Check
        if any(crit_dir in file_path.parts for crit_dir in CRITICAL_DIRS):
            # More nuanced: allow some cleanup in "scripts" if file is clearly temp
            if "scripts" in file_path.parts and not any(p.match(file_path.name) for p in POTENTIALLY_OBSOLETE_NAME_PATTERNS):
                 pass # Allow script files to be evaluated further
            else:
                return False, f"In critical directory \'{relative_path_str}\'"

        # 2. Protected Pattern Check
        if any(pattern.search(file_path.name) for pattern in PROTECTED_PATTERNS):
            return False, f"Matches protected pattern \'{file_path.name}\'"

        # 3. Scheduled Script Check
        if str(file_path) in self.automation_scripts:
            return False, f"Is a scheduled automation script \'{relative_path_str}\'"

        # 4. Explicit Expiration (from inventory or .cleanup_registry.json)
        expiration_str = file_info.get(\'expiration\', \'none\')
        if expiration_str == \'none\' and str(file_path) in self.cleanup_registry:
            expiration_str = self.cleanup_registry[str(file_path)].get(\'expires\')
        
        if expiration_str and expiration_str != \'none\':
            try:
                # Attempt to parse various common date formats
                exp_date = datetime.fromisoformat(expiration_str.replace(\'Z\', \'+00:00\'))
                if exp_date.tzinfo is None: # Assume UTC if no timezone
                    exp_date = exp_date.replace(tzinfo=timezone.utc)
                if datetime.now(timezone.utc) < exp_date:
                    return False, f"Not yet expired (expires {expiration_str})"
            except ValueError:
                logging.warning(f"Could not parse expiration date \'{expiration_str}\' for {file_path}")
                # If unparsable, treat as non-expiring for safety unless other flags apply
                # return False, f"Unparsable expiration date \'{expiration_str}\'" # Or be more lenient

        # 5. Git Tracking & Age & References (for untracked files or non-critical tracked files)
        is_git_tracked = file_info.get(\'git_tracked\', False)
        modified_time = datetime.fromtimestamp(int(file_info.get(\'modified_epoch\', 0)), timezone.utc)
        age_days = (datetime.now(timezone.utc) - modified_time).days
        references = int(file_info.get(\'references\', 0))

        # Rule: Old, untracked, unreferenced files are strong candidates
        if not is_git_tracked and age_days > 90 and references == 0:
             if any(p.match(file_path.name) for p in POTENTIALLY_OBSOLETE_NAME_PATTERNS) or \
                file_path.suffix in POTENTIALLY_OBSOLETE_EXTENSIONS:
                return True, f"Old ({age_days}d), untracked, 0 refs, name/ext matches temp pattern"

        # Rule: Untracked files with temp patterns/extensions, older than 7 days
        if not is_git_tracked and age_days > 7 and \
           (any(p.match(file_path.name) for p in POTENTIALLY_OBSOLETE_NAME_PATTERNS) or \
            file_path.suffix in POTENTIALLY_OBSOLETE_EXTENSIONS):
            return True, f"Untracked, {age_days}d old, name/ext matches temp pattern"
        
        # Rule: Tracked files that look like temporary output, very old, and few refs
        # Be more careful with tracked files
        if is_git_tracked and age_days > 180 and references <= 1 and \
           (any(p.match(file_path.name) for p in POTENTIALLY_OBSOLETE_NAME_PATTERNS) or \
            file_path.suffix in POTENTIALLY_OBSOLETE_EXTENSIONS):
             # Ensure it\'s not a core script file unless it really looks like an old artifact
            if "scripts/" in relative_path_str and not any(p.match(file_path.name) for p in POTENTIALLY_OBSOLETE_NAME_PATTERNS):
                 return False, f"Tracked script, needs manual review despite age/refs: {relative_path_str}"
            return True, f"Tracked, very old ({age_days}d), low refs ({references}), looks like temp/output"

        # Rule: Files in .cleanup_registry that are past expiration
        if str(file_path) in self.cleanup_registry:
            entry = self.cleanup_registry[str(file_path)]
            if entry.get(\'expires\'):
                try:
                    exp_dt = datetime.fromisoformat(entry[\'expires\'].replace(\'Z\', \'+00:00\'))
                    if exp_dt.tzinfo is None: exp_dt = exp_dt.replace(tzinfo=timezone.utc)
                    if datetime.now(timezone.utc) > exp_dt:
                        return True, f"Registered for cleanup and past expiration ({entry[\'expires\']})"
                except ValueError: pass # Already logged

        return False, "No specific cleanup rule matched"

    def interactive_cleanup(self):
        """Interactive cleanup with user confirmation."""
        candidates_for_review = []
        for file_info in self.inventory:
            is_safe, reason = self.is_safe_to_remove(file_info)
            if is_safe:
                candidates_for_review.append((file_info, reason))
        
        if not candidates_for_review:
            logging.info("No files flagged for cleanup based on current rules.")
            return

        logging.info(f"Found {len(candidates_for_review)} files for potential cleanup review:")
        
        deleted_count = 0
        kept_count = 0

        for file_info, reason in candidates_for_review:
            path = Path(file_info[\'path\'])
            size_kb = int(file_info.get(\'size\', 0)) / 1024
            mod_date = datetime.fromtimestamp(int(file_info.get(\'modified_epoch\', 0)), timezone.utc).strftime(\'%Y-%m-%d %H:%M:%S %Z\')
            
            print("-" * 50)
            print(f"ðŸ“ File: {path.relative_to(self.project_root)}")
            print(f"   Reason: {reason}")
            print(f"   Size: {size_kb:.2f} KB, Modified: {mod_date}")
            print(f"   Type: {file_info.get(\'type\', \'N/A\')}, Git Tracked: {file_info.get(\'git_tracked\', False)}")
            print(f"   References (approx): {file_info.get(\'references\', 0)}")
            
            if self.dry_run:
                print("   Action: Would be prompted for deletion (Dry Run Mode)")
                continue

            while True:
                choice = input("   Action [d(elete)/k(eep)/v(iew)/s(kip this session)]? ").lower()
                if choice == \'d\':
                    self._perform_delete(file_info)
                    deleted_count +=1
                    break
                elif choice == \'k\':
                    logging.info(f"Kept file: {path}")
                    kept_count += 1
                    break
                elif choice == \'v\':
                    self._view_file(path)
                elif choice == \'s\':
                    logging.info("Skipping further interactive review in this session.")
                    print(f"\nSummary: Deleted {deleted_count} files, Kept (reviewed) {kept_count} files in this session.")
                    return # Exit interactive loop
                else:
                    print("Invalid choice. Please use d/k/v/s.")
        
        print(f"\nCleanup Review Complete. Deleted: {deleted_count}, Kept (reviewed): {kept_count}")

    def _view_file(self, file_path: Path):
        try:
            content = file_path.read_text(errors=\'ignore\')
            print("\n--- File Content (first 500 chars) ---")
            print(content[:500])
            if len(content) > 500:
                print("... (file truncated)")
            print("--- End File Content ---\n")
        except Exception as e:
            print(f"Error viewing file {file_path}: {e}")

    def _perform_delete(self, file_info: dict):
        path = Path(file_info[\'path\'])
        log_message = f"File: {path.relative_to(self.project_root)}, Size: {file_info.get(\'size\',0)}, Git: {file_info.get(\'git_tracked\')}"
        
        if self.dry_run:
            logging.info(f"[DRY RUN] Would delete: {log_message}")
            return

        try:
            if file_info.get(\'git_tracked\', False):
                # For git-tracked files, prefer `git rm`
                subprocess.run([\'git\', \'rm\', str(path)], check=True, cwd=self.project_root)
                logging.info(f"Git removed: {log_message}")
            else:
                path.unlink() # Deletes the file
                logging.info(f"Deleted: {log_message}")
            
            # Remove from cleanup_registry if it was there
            if str(path) in self.cleanup_registry:
                del self.cleanup_registry[str(path)]
                self._save_json(self.project_root / CLEANUP_REGISTRY_FILE, self.cleanup_registry)

        except Exception as e:
            logging.error(f"Error deleting {path}: {e}")
    
    def _save_json(self, file_path: Path, data: any):
        try:
            with file_path.open(\'w\') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logging.error(f"Error saving JSON to {file_path}: {e}")


    @staticmethod
    def transient_file_decorator_example(): # For documentation
        """
        This shows how a transient_file decorator might work.
        It should be implemented in a shared utility module.
        """
        from functools import wraps
        import atexit

        def register_for_cleanup(filepath: Path, expiration_dt: datetime):
            registry_path = Path(IntelligentCleanup.project_root / CLEANUP_REGISTRY_FILE if hasattr(IntelligentCleanup, \'project_root\') else CLEANUP_REGISTRY_FILE) # Adjust as needed
            current_registry = {}
            if registry_path.exists():
                with registry_path.open(\'r\') as f:
                    try:
                        current_registry = json.load(f)
                    except json.JSONDecodeError: pass # ignore if malformed, will overwrite
            
            current_registry[str(filepath.resolve())] = {
                "expires": expiration_dt.isoformat(),
                "registered_at": datetime.now(timezone.utc).isoformat()
            }
            with registry_path.open(\'w\') as f:
                json.dump(current_registry, f, indent=2)
            logging.info(f"Registered {filepath} for cleanup, expires {expiration_dt.isoformat()}")

        def transient_file(lifetime_hours: int = 72):
            def decorator(func):
                @wraps(func)
                def wrapper(*args, **kwargs):
                    # Assuming the decorated function returns a Path object or string path to the created file
                    file_path_result = func(*args, **kwargs)
                    
                    if file_path_result:
                        p_result = Path(file_path_result)
                        if p_result.exists(): # Ensure file was actually created
                            expiration_datetime = datetime.now(timezone.utc) + timedelta(hours=lifetime_hours)
                            register_for_cleanup(p_result, expiration_datetime)
                            # Optionally, write expiration into the file itself if it\'s text.
                            # if p_result.suffix in [\'.txt\', \'.log\', \'.md\', \'.json\', \'.yaml\']:
                            #    with p_result.open(\'a\') as f: # Appending might not be ideal
                            #        f.write(f"\n# This file expires around: {expiration_datetime.isoformat()}")
                        else:
                            logging.warning(f"Transient file decorator: function {func.__name__} returned path {p_result} but file does not exist.")
                    return file_path_result
                return wrapper
            return decorator

        # Example Usage (conceptual):
        # from core.utils import transient_file # Assuming decorator is placed there
        # @transient_file(lifetime_hours=24)
        # def create_temp_report(data) -> Path:
        #     report_path = Path(f"temp_report_{datetime.now().strftime(\'%Y%m%d%H%M%S\')}.txt")
        #     report_path.write_text(str(data))
        #     return report_path


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python cleanup_engine.py <path_to_inventory.json> [--execute]")
        print("Example (dry run): python scripts/cleanup_engine.py cleanup_inventory.json")
        print("Example (execute): python scripts/cleanup_engine.py cleanup_inventory.json --execute")
        sys.exit(1)

    inventory_fpath = sys.argv[1]
    is_dry_run = "--execute" not in sys.argv
    
    # Infer project root from inventory file path (assuming inventory is in project root)
    proj_root = str(Path(inventory_fpath).resolve().parent)

    engine = IntelligentCleanup(inventory_file_path=inventory_fpath, project_root_path=proj_root, dry_run=is_dry_run)
    engine.interactive_cleanup()
    logging.info(f"Cleanup {\'DRY RUN \' if is_dry_run else \'\'}session complete. Log: {LOG_FILE.resolve()}")

```
**Key features of `cleanup_engine.py`:**
*   Takes the `cleanup_inventory.json` path as input.
*   Runs in dry-run mode by default; requires `--execute` to perform deletions.
*   Sophisticated `is_safe_to_remove` logic considering:
    *   Critical directories and protected file patterns.
    *   Known scheduled automation scripts.
    *   Explicit expiration dates (from file content or a `.cleanup_registry.json`).
    *   Git tracking status, file age, and reference count.
    *   Temporary name/extension patterns.
*   Interactive prompt for each candidate file (`delete/keep/view/skip session`).
*   Logs all actions to `cleanup_actions.log`.
*   If deleting a Git-tracked file, uses `git rm`.
*   Includes an example `transient_file` decorator concept for proactive lifecycle management.

### Phase 3: Automated Script Management (Proactive Approach)

This involves ensuring that legitimate scripts that *are* needed for automation are properly managed, scheduled, and monitored, distinguishing them from one-off obsolete scripts.

**Script: `scripts/automation_manager.py` (Conceptual - for registering and managing automated tasks)**
```python
#!/usr/bin/env python3
# automation_manager.py - Manage legitimate automation scripts for Project Symphony

import yaml
import os
import subprocess
import logging
from pathlib import Path
from datetime import datetime
from croniter import croniter # requires: pip install croniter

AUTOMATION_CONFIG_FILE = Path("config/automation_registry.yaml")
MANAGED_SCRIPTS_DIR = Path("scripts/automation/") # Store managed scripts here
LOG_DIR = Path("logs/automation/")
HEALTH_DIR = Path("status/automation_health/")

# Ensure directories exist
MANAGED_SCRIPTS_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)
HEALTH_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(level=logging.INFO, format=\'%(asctime)s [%(levelname)s] %(message)s\')

class AutomationManager:
    def __init__(self):
        self.config_path = AUTOMATION_CONFIG_FILE
        self.registry = self._load_registry()

    def _load_registry(self) -> dict:
        if self.config_path.exists():
            with self.config_path.open(\'r\') as f:
                try:
                    return yaml.safe_load(f) or {\'managed_scripts\': {}}
                except yaml.YAMLError as e:
                    logging.error(f"Error loading automation registry {self.config_path}: {e}")
                    return {\'managed_scripts\': {}}
        return {\'managed_scripts\': {}}

    def _save_registry(self):
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with self.config_path.open(\'w\') as f:
            yaml.dump(self.registry, f, sort_keys=False)
        logging.info(f"Automation registry saved to {self.config_path}")

    def register_script(self, script_name: str, original_path_str: str, schedule: str, description: str, owner: str):
        original_path = Path(original_path_str).resolve()
        if not original_path.exists():
            logging.error(f"Script to register not found: {original_path}")
            return False

        if not croniter.is_valid(schedule):
            logging.error(f"Invalid cron schedule \'{schedule}\' for script \'{script_name}\'")
            return False

        managed_script_path = (MANAGED_SCRIPTS_DIR / original_path.name).resolve()
        
        # Copy script to managed directory (optional, could also just reference)
        # shutil.copy(original_path, managed_script_path)
        # os.chmod(managed_script_path, 0o755)
        # For this example, we\'ll assume it\'s referenced if already in a scripts dir, or copied if elsewhere.
        # Simpler: we expect scripts to be placed in `scripts/automation/` by the user.
        managed_script_path = original_path # Assuming it\'s already in a suitable project location.

        script_id = script_name.lower().replace(" ", "_")
        if script_id in self.registry[\'managed_scripts\']:
            logging.warning(f"Script \'{script_id}\' already registered. Use \'update_script\' or choose a new name.")
            return False

        self.registry[\'managed_scripts\'][script_id] = {
            \'name\': script_name,
            \'path\': str(managed_script_path), # Store relative to project root if possible, or absolute
            \'schedule\': schedule,
            \'description\': description,
            \'owner\': owner,
            \'registered_at\': datetime.now().isoformat(),
            \'last_run_status\': \'not_run_yet\',
            \'health_check_file\': str((HEALTH_DIR / f"{script_id}.health").resolve()),
            \'log_file\': str((LOG_DIR / f"{script_id}.log").resolve()),
            \'is_active\': True
        }
        self._save_registry()
        logging.info(f"Registered automation script \'{script_name}\' (ID: {script_id}). Schedule: {schedule}")
        
        # Here you would integrate with actual system scheduler (cron, systemd)
        # This is a complex step and platform-dependent.
        # For cron, you might generate a cron snippet.
        self._generate_cron_entry(script_id, self.registry[\'managed_scripts\'][script_id])
        return True

    def _generate_cron_entry(self, script_id: str, script_config: dict):
        # This is a simplified example. Real cron management is harder.
        # It creates a wrapper to handle logging and health checks.
        
        script_path = Path(script_config[\'path\'])
        health_file = Path(script_config[\'health_check_file\'])
        log_file = Path(script_config[\'log_file\'])
        schedule = script_config[\'schedule\']
        
        # Create a wrapper script for execution via cron
        wrapper_script_path = MANAGED_SCRIPTS_DIR / f"wrapper_{script_id}.sh"
        wrapper_content = f"""#!/bin/bash
# Auto-generated wrapper for {script_id}
set -euo pipefail
SCRIPT_TO_RUN="{script_path.resolve()}"
HEALTH_FILE="{health_file.resolve()}"
LOG_FILE="{log_file.resolve()}"
PROJECT_ROOT="{script_path.parent.parent.resolve()}" # Assuming script is in scripts/automation

echo "----------------------------------------" >> "$LOG_FILE"
echo "$(date \'+\%Y-\%m-\%d \%H:\%M:\%S \%Z\') - Starting $SCRIPT_TO_RUN" >> "$LOG_FILE"

# Ensure Python scripts use project\'s venv if applicable
# Example: source "$PROJECT_ROOT/venv/bin/activate" && python "$SCRIPT_TO_RUN" >> "$LOG_FILE" 2>&1
# Or, if the script is executable and has a shebang:
if "$SCRIPT_TO_RUN" >> "$LOG_FILE" 2>&1; then
    STATUS="success"
    echo "$(date \'+\%Y-\%m-\%d \%H:\%M:\%S \%Z\') - SUCCESS" >> "$LOG_FILE"
else
    STATUS="failed"
    echo "$(date \'+\%Y-\%m-\%d \%H:\%M:\%S \%Z\') - FAILED (Exit Code: $?)" >> "$LOG_FILE"
fi
echo "$STATUS:$(date \'+\%Y-\%m-\%d \%H:\%M:\%S \%Z\')" > "$HEALTH_FILE"
echo "----------------------------------------" >> "$LOG_FILE"
exit 0 # Wrapper always exits 0 to prevent cron emails on script failure; check health file instead
"""
        with wrapper_script_path.open(\'w\') as f:
            f.write(wrapper_content)
        os.chmod(wrapper_script_path, 0o755)

        cron_job_line = f"{schedule} {wrapper_script_path.resolve()} # Symphony Automation ID: {script_id}"
        logging.info(f"To activate, add to crontab: {cron_job_line}")
        # In a real system, you might use python-crontab or call `crontab -e`
        # Or generate a file for /etc/cron.d/
        print(f"\n--- Suggested Crontab Entry for {script_id} ---")
        print(cron_job_line)
        print("--------------------------------------------")
        return cron_job_line

    def list_scripts(self):
        if not self.registry[\'managed_scripts\']:
            print("No automation scripts registered.")
            return
        print("Registered Automation Scripts:")
        for script_id, details in self.registry[\'managed_scripts\'].items():
            print(f"  ID: {script_id}, Name: {details[\'name\']}, Schedule: {details[\'schedule\']}, Active: {details.get(\'is_active\', True)}")
            print(f"    Path: {details[\'path\']}")
            print(f"    Last Status: {details.get(\'last_run_status\', \'N/A\')}")

    # ... (add methods for update_script, deactivate_script, check_health, etc.) ...

# Example script template to be placed in scripts/automation/
AUTOMATION_SCRIPT_TEMPLATE = \'\'\'#!/usr/bin/env python3
# Automation Script: {script_name}
# Description: {description}
# Schedule: {schedule} (Managed by AutomationManager)
# Owner: {owner}
# Registered: {created_date}

import sys
import logging
from pathlib import Path

# Setup basic logging for this script\'s execution (will be captured by wrapper)
# The wrapper script manages the primary log file.
# This internal logging is for detailed script steps.
logging.basicConfig(
    level=logging.INFO, # Or DEBUG
    format=\'%(asctime)s - %(levelname)s - %(module)s - %(message)s\',
    stream=sys.stdout # Output will be redirected by wrapper
)

def main():
    script_name = Path(__file__).name
    logging.info(f"Starting automation task: {script_name}")
    try:
        # --- YOUR AUTOMATION LOGIC HERE ---
        # Example:
        # print("Performing automated task...")
        # for i in range(5):
        #    logging.debug(f"Step {i+1}")
        #    time.sleep(1)
        # if some_condition_fails:
        #    logging.error("A critical condition failed.")
        #    return 1 # Indicate failure

        logging.info(f"Completed automation task: {script_name} successfully.")
        return 0 # Indicate success
        
    except Exception as e:
        logging.error(f"Error during automation task {script_name}: {e}", exc_info=True)
        return 1 # Indicate failure

if __name__ == "__main__":
    sys.exit(main())
\'\'\'
# To use this manager:
# manager = AutomationManager()
# manager.register_script(
#     script_name="Nightly Data Sync",
#     original_path_str="scripts/my_data_sync_script.py", # Must exist
#     schedule="0 2 * * *", # Every day at 2 AM
#     description="Synchronizes data from source X to target Y.",
#     owner="data_team"
# )
# manager.list_scripts()
```
**Key features of `automation_manager.py`:**
*   Maintains a YAML registry (`config/automation_registry.yaml`) of "official" automation scripts.
*   Provides functions to register new scripts with metadata (schedule, description, owner).
*   Generates a wrapper script and a cron job line for each registered script. The wrapper handles standardized logging and health status reporting.
*   This system helps distinguish actively managed, necessary automation from random, unmanaged scripts.
*   **Note:** Actual integration with system schedulers (cron, systemd) is complex and platform-dependent. This script provides a starting point by generating cron lines.

## Part 4: Automated Script Management & Scheduling Strategy

This section details how to reliably schedule and manage the cleanup scripts and other necessary automation tasks, drawing from best practices.

### Primary Scheduling Tools: `cron` and `systemd` Timers

*   **`cron`:** Ideal for time-based, recurring tasks (e.g., daily at 3 AM). Simple and widely available.
*   **`systemd` timers:** More flexible, can trigger on events (boot, specific service start), calendar events (similar to cron but more powerful), and offer better logging/management via `journalctl`. Preferred for critical system tasks or tasks needing complex dependencies.
*   **`anacron`:** Ensures that jobs missed due to system downtime (e.g., a laptop being off) are run when the system is next available. Often used for user-level cron jobs or non-critical daily/weekly tasks on systems that aren\'t always on.

### Proposed Schedule Framework

Paths are illustrative; adapt them to your project structure (e.g., `/opt/cherry_ai-main/scripts/` or use relative paths if cron is run as the project user from the project root). Ensure scripts are executable.

**1. Daily Maintenance (via `cron`)**
   *   **Purpose:** Routine cleanup of clearly temporary files, log rotation.
   *   **When:** Low-load hours (e.g., 3:00 - 4:00 AM server time).
   *   **Example Crontab Entry (e.g., in `/etc/cron.d/cherry_ai-maintenance` or user\'s crontab):**
     ```cron
     # Project Symphony - Daily Maintenance
     SHELL=/bin/bash
     PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
     PROJECT_ROOT=/root/cherry_ai-main # ADJUST THIS

     # At 3:05 AM: Run the intelligent cleanup engine in a non-interactive, less aggressive mode
     # This assumes cleanup_engine.py can take flags for non-interactive, specific rule sets.
     # For now, this might just run the inventory to update it.
     05 3 * * * root cd $PROJECT_ROOT && ./scripts/comprehensive_inventory.sh >> $PROJECT_ROOT/logs/inventory_daily.log 2>&1
     
     # At 3:15 AM: (If cleanup_engine.py supported a non-interactive mode for very safe deletions)
     # 15 3 * * * root cd $PROJECT_ROOT && python ./scripts/cleanup_engine.py $PROJECT_ROOT/cleanup_inventory.json --non-interactive --ruleset=daily_safe_deletions >> $PROJECT_ROOT/logs/cleanup_daily.log 2>&1

     # At 3:30 AM: Rotate application logs (if not handled by logging framework or system logrotate)
     # 30 3 * * * root cd $PROJECT_ROOT && python ./scripts/rotate_app_logs.py >> $PROJECT_ROOT/logs/log_rotation.log 2>&1
     ```
   *   **Associated Script(s):**
        *   `scripts/comprehensive_inventory.sh` (to keep inventory fresh for checks).
        *   (Future) `scripts/cleanup_engine.py --non-interactive --ruleset=daily_safe_deletions` (if enhanced for this).
        *   A custom log rotation script if needed: `scripts/rotate_app_logs.py`.

**2. Weekly Deep Clean & Review (via `cron` or `systemd` timer)**
   *   **Purpose:** More thorough analysis, prompting for review of aging/unreferenced files. Generate reports.
   *   **When:** Weekend, low-load hours (e.g., Sunday 2:00 AM).
   *   **Example Crontab Entry:**
     ```cron
     # Project Symphony - Weekly Deep Clean
     # Sunday at 2:00 AM: Update inventory
     0 2 * * 0 root cd $PROJECT_ROOT && ./scripts/comprehensive_inventory.sh >> $PROJECT_ROOT/logs/inventory_weekly.log 2>&1
     # Sunday at 2:15 AM: Run cleanup engine in interactive mode (output to a log for later review if not truly interactive)
     # This is tricky for cron. Better to run manually or have it generate a report of candidates.
     # Option 1: Generate a report of candidates
     15 2 * * 0 root cd $PROJECT_ROOT && python ./scripts/cleanup_engine.py $PROJECT_ROOT/cleanup_inventory.json --report-only > $PROJECT_ROOT/reports/weekly_cleanup_candidates.txt
     # Option 2: (If interactive is needed) - this requires someone to be there or a very careful non-interactive mode.
     ```
   *   **Associated Script(s):**
        *   `scripts/comprehensive_inventory.sh`.
        *   `scripts/cleanup_engine.py` (run manually with this fresh inventory, or in a `--report-only` mode).

**3. Real-time/Frequent Monitoring (via `systemd` timer if high frequency needed, or less frequent cron)**
    *   **Purpose:** Check for rapid proliferation of temp files, monitor health of automation scripts.
    *   **When:** Every 1-4 hours during workdays, or less frequently.
    *   **Example `systemd` Timer (`/etc/systemd/system/symphony-quickcheck.timer`):**
      ```systemd
      [Unit]
      Description=Project Symphony Quick Health Check Timer
      
      [Timer]
      OnCalendar=hourly  # Or Mon-Fri *:00/4:00:00 for every 4 hours on workdays
      Persistent=true    # Run on next boot if missed
      
      [Install]
      WantedBy=timers.target
      ```
    *   **Associated `systemd` Service (`/etc/systemd/system/symphony-quickcheck.service`):**
      ```systemd
      [Unit]
      Description=Project Symphony Quick Health Check Service
      
      [Service]
      Type=oneshot
      User=your_project_user # Run as appropriate user
      WorkingDirectory=/root/cherry_ai-main # ADJUST THIS
      ExecStart=/root/cherry_ai-main/scripts/quick_health_check.sh # ADJUST THIS
      StandardOutput=journal+console 
      ```
    *   **Associated Script (`scripts/quick_health_check.sh`):**
      ```bash
      #!/bin/bash
      PROJECT_ROOT=/root/cherry_ai-main # ADJUST THIS
      LOG_FILE="$PROJECT_ROOT/logs/quick_health.log"
      echo "$(date): Running quick health check..." >> "$LOG_FILE"
      
      # Check count of files matching temp patterns in specific dirs (e.g., uploads, processing output)
      TEMP_FILE_COUNT=$(find "$PROJECT_ROOT/var/temp_processing" -type f -name "tmp_*" -mmin -120 | wc -l) # Temp files in last 2 hours
      if [ "$TEMP_FILE_COUNT" -gt 50 ]; then
          echo "WARNING: High number of recent temp files found: $TEMP_FILE_COUNT" >> "$LOG_FILE"
          # Add notification logic here
      fi

      # Check health of registered automation scripts (using health files from AutomationManager)
      find "$PROJECT_ROOT/status/automation_health" -name "*.health" -type f | while read -r health_file; do
          status=$(cat "$health_file")
          if [[ "$status" == failed* ]]; then
              echo "ERROR: Automation script $(basename "$health_file" .health) reported failure: $status" >> "$LOG_FILE"
              # Add notification logic
          fi
      done
      echo "$(date): Quick health check complete." >> "$LOG_FILE"
      ```

**4. Git Repository Maintenance (via `cron`, less frequently)**
   *   **Purpose:** `git gc`, prune remote branches, etc.
   *   **When:** Monthly, or as needed.
   *   **Example Crontab Entry:**
     ```cron
     # Project Symphony - Monthly Git Maintenance
     0 4 1 * * root cd $PROJECT_ROOT && git gc --aggressive --prune=now && git remote prune origin >> $PROJECT_ROOT/logs/git_maintenance.log 2>&1
     ```

### Health Checks and Notifications
*   **Health Files:** Each managed automation script (via `AutomationManager` or similar custom wrappers) should write to a status/health file upon completion (e.g., `status/automation_health/script_name.health` containing `success:timestamp` or `failed:timestamp`).
*   **Monitoring Script:** A separate cron job (e.g., hourly) can run `scripts/quick_health_check.sh` to parse these health files and a configured alerting mechanism (email, Slack, etc.) if failures are detected.
*   **Logging:** All scheduled scripts MUST log their output (stdout and stderr) to dedicated log files (e.g., in `logs/cron/` or `logs/automation/`) with timestamps. Log rotation should be configured for these.

### Best Practices for Scheduled Scripts
*   **Idempotency:** Scripts that modify state should be idempotent (running them multiple times has the same effect as running them once).
*   **Error Handling:** Robust error handling within scripts. Exit codes should accurately reflect success/failure.
*   **Resource Limits:** For potentially intensive scripts, use `nice` and `ionice` to lower their priority. Consider timeout mechanisms (e.g., `timeout` command) for scripts that might hang.
*   **Locking:** For scripts that should not run concurrently, implement a locking mechanism (e.g., using `flock` or creating/checking for a lock file).
*   **Configuration:** Avoid hardcoding paths or credentials. Use a central configuration file or environment variables.
*   **Permissions:** Run scripts with the minimum necessary permissions.
*   **Testing:** Test scheduled scripts thoroughly in a staging environment before deploying to production.

## Part 5: Continuous Monitoring & Optimization

### 1. GitHub Actions Workflow (Example)

**File: `.github/workflows/ai_codebase_hygiene.yml`**
```yaml
name: AI Codebase Hygiene & Performance Monitoring

on:
  schedule:
    - cron: "0 2 * * 0"  # Weekly, Sunday at 2 AM UTC
  push:
    branches: [main, develop] # Or your primary branches
  pull_request:
    branches: [main, develop]

jobs:
  hygiene-and-linting:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Needed for git blame/log analysis if used by scripts

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: \'3.10\' # Match project version
          cache: \'pip\'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt # Or specific dev/test requirements
          # pip install black flake8 isort bandit # Linters and formatters
          # pip install -r requirements/dev.txt # if you have dev requirements

      - name: Run Linters and Formatters
        run: |
          echo "Running Black formatter check..."
          black --check .
          echo "Running Flake8 linter..."
          flake8 .
          # echo "Running isort import sorter check..."
          # isort --check-only .
          # echo "Running Bandit security linter..."
          # bandit -r .

      - name: Run File Inventory Script
        id: inventory
        run: |
          ./scripts/comprehensive_inventory.sh
          echo "Inventory generated: cleanup_inventory.json"
          # Upload inventory as artifact
          mkdir -p reports
          cp cleanup_inventory.json reports/cleanup_inventory_$(date +%Y%m%d).json

      - name: Analyze Inventory (Dry Run Cleanup Check)
        if: steps.inventory.outcome == \'success\'
        run: |
          # Assumes cleanup_engine.py can output a report or a list of candidates in dry run
          python ./scripts/cleanup_engine.py cleanup_inventory.json --dry-run > reports/cleanup_candidates_$(date +%Y%m%d).txt
          echo "Cleanup candidate report generated."

      - name: Upload Reports Artifact
        uses: actions/upload-artifact@v4
        with:
          name: codebase-hygiene-reports-${{ github.run_id }}
          path: |
            reports/
            cleanup_actions.log # If generated by cleanup_engine
          retention-days: 30

  # Optional: Performance Analysis (if you have benchmark scripts)
  # performance-analysis:
  #   runs-on: ubuntu-latest
  #   needs: hygiene-and-linting # Run after basic checks
  #   steps:
  #     - uses: actions/checkout@v4
  #     - uses: actions/setup-python@v4
  #       with:
  #         python-version: \'3.10\'
  #         cache: \'pip\'
  #     - name: Install dependencies
  #       run: pip install -r requirements.txt # Or benchmark-specific requirements
  #     - name: Run performance benchmarks
  #       run: |
  #         # python -m pytest tests/benchmarks/ # Example
  #         echo "Performance benchmarks would run here."
  #         # Store results in a file for upload
  #         mkdir -p reports/performance
  #         echo "Benchmark results..." > reports/performance/benchmark_summary_$(date +%Y%m%d).txt
  #     - name: Upload Performance Reports Artifact
  #       uses: actions/upload-artifact@v4
  #       with:
  #         name: performance-reports-${{ github.run_id }}
  #         path: reports/performance/
  #         retention-days: 90

```

### 2. Performance Monitoring Dashboard (Conceptual)

While a full dashboard setup is beyond this scope, here are metrics to track, which could be fed into Grafana, Prometheus, or a custom solution:

*   **Codebase Health:**
    *   **Number of files flagged by `cleanup_engine.py` (trend over time):** Indicates effectiveness of Zero Junk policy.
    *   **Average age of files in `temp/` or matching temp patterns:** Should remain low.
    *   **Technical Debt Ratio:** (If using tools like SonarQube) Track this.
    *   **Linter/Formatter Violations:** Number of issues reported by Black, Flake8.
*   **AI Generation Metrics:**
    *   **Rate of new standalone script generation (manual tracking or via commit analysis):** Should trend downwards.
    *   **Number of files created with `transient_file` decorator or registered in `.cleanup_registry.json`:** Indicates adoption of lifecycle management.
*   **System Performance (related to AI-generated components):**
    *   **P99 Latency for critical API endpoints/services impacted by AI code.**
    *   **Memory/CPU usage of key AI-driven services.**
    *   **Error rates for these services.**
*   **Automation Health:**
    *   **Success/failure rate of registered automation scripts (from health check files).**
    *   **Duration of cleanup script runs.**

**Data Sources:**
*   Output from `comprehensive_inventory.sh` and `cleanup_engine.py`.
*   Linter and formatter tools.
*   Version control history (for script creation patterns).
*   Application Performance Monitoring (APM) tools.
*   Log files from scheduled tasks.

## Part 6: Implementation Checklist

**Phase 1: Setup & Initial Analysis (Weeks 1-2)**
*   [ ] **Review & Customize:** Thoroughly review this entire framework document with the team. Customize `CRITICAL_DIRS`, `PROTECTED_PATTERNS`, and script paths.
*   [ ] **Tool Configuration:** Update Roo, Cursor, and any other AI assistant configurations with the new rules and guidelines from Part 2.
*   [ ] **Deploy Core Scripts:**
    *   [ ] Add `scripts/comprehensive_inventory.sh` to the project. Test it.
    *   [ ] Add `scripts/cleanup_engine.py` to the project. Test its dry-run mode.
    *   [ ] Add `scripts/automation_manager.py` (if adopting this for new scripts).
*   [ ] **Initial Inventory Run:** Execute `comprehensive_inventory.sh` to get a baseline understanding of the current state.
*   [ ] **Initial Cleanup Review (Dry Run):** Run `cleanup_engine.py` in dry-run mode with the generated inventory. Analyze the flagged files with the team.
*   [ ] **Develop `transient_file` Decorator:** Implement and test the `transient_file` decorator (or similar mechanism) in a shared utility module.

**Phase 2: First Cleanup & Process Integration (Weeks 3-4)**
*   [ ] **Manual Review & First Cleanup:** Based on the dry run, perform the first interactive cleanup session using `cleanup_engine.py --execute`.
*   [ ] **AI Assistant Training:** Communicate the new guidelines clearly to all developers using AI assistants. Emphasize the "Zero Junk Policy" and integration preference.
*   [ ] **Integrate `transient_file`:** Start applying the `transient_file` decorator to new AI-generated code that produces temporary outputs.
*   [ ] **Setup Basic Scheduling:** Implement daily cron job for `comprehensive_inventory.sh` and a weekly placeholder for `cleanup_engine.py` review/report.
*   [ ] **Setup GitHub Actions:** Implement the `ai_codebase_hygiene.yml` workflow.

**Phase 3: Ongoing Monitoring & Refinement (Months 2-3 onwards)**
*   [ ] **Regular Cleanup Sessions:** Schedule and conduct regular (e.g., bi-weekly or monthly) cleanup reviews using `cleanup_engine.py`.
*   [ ] **Monitor GitHub Actions Reports:** Review reports from the hygiene workflow. Identify patterns and areas for improvement.
*   [ ] **Refine AI Guidance:** Based on observed AI outputs and developer feedback, refine the rules and prompts for Roo, Cursor, etc.
*   [ ] **Expand `AutomationManager` Usage:** Register all new legitimate automation scripts via the `AutomationManager`.
*   [ ] **Develop Monitoring Dashboard Data Collection:** Start scripting the collection of key metrics for future dashboarding.
*   [ ] **Quarterly Review:** Conduct a quarterly review of the framework\'s effectiveness and make adjustments as needed.

## Part 7: Success Metrics

*   **Codebase Clutter Reduction:**
    *   **Decrease in untracked/unreferenced files:** >50% reduction in files flagged by `cleanup_engine.py` (excluding newly generated valid temp files with lifecycle) within 3 months.
    *   **Average age of files in `temp/` directories:** Maintained below 7 days.
*   **Adherence to "Zero Junk Policy":**
    *   **Standalone script creation rate:** < 1 per month (unless explicitly part of a documented, temporary workflow) after 2 months.
    *   **Files with managed lifecycle:** >80% of new AI-generated temporary files use `transient_file` or are registered in `.cleanup_registry.json` within 3 months.
*   **Performance & Efficiency (Indirectly influenced by cleaner codebase):**
    *   **P99 Latency:** Maintain or improve P99 latency for key services.
    *   **Build/Deployment Times:** Observe for any improvements due to a leaner repository.
*   **Developer Productivity & Code Quality:**
    *   **Reduced time spent finding relevant code:** (Qualitative feedback from developers).
    *   **Improved clarity of repository structure:** (Qualitative feedback).
    *   **Stable or decreasing technical debt score** (if measured).
*   **Automation Reliability:**
    *   **Success rate of registered automation scripts:** >99%.
    *   **Incidents related to obsolete/conflicting scripts:** Reduced to near zero.

This comprehensive framework provides a roadmap for achieving a cleaner, more performant, and more manageable codebase through a combination of intelligent automation, clear AI guidance, and diligent process adherence. It will require ongoing effort and team commitment but will yield significant long-term benefits. 