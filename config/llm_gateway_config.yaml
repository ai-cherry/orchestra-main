## AI Orchestra Gateway Configuration

# This config sets up Portkey as our primary LLM gateway with Kong as a fallback option
# Based on enterprise requirements for security, compliance, and performance

# General Gateway Configuration
llm_gateway:
  primary_provider: "portkey" # Options: portkey, openrouter, kong, litellm
  fallback_provider: "kong" # Secondary gateway if primary fails
  timeout_seconds: 15 # Request timeout
  enable_caching: true # Enable semantic caching
  cache_ttl_seconds: 3600 # Cache lifetime
  enable_circuit_breaker: true # Auto-disable failing providers temporarily
  failover_retries: 3 # Retry attempts before switching providers

# Portkey Configuration (Primary)
portkey:
  api_key: "${PORTKEY_API_KEY}"
  base_url: "https://api.portkey.ai/v1/gateway"
  virtual_keys:
    openai: "${PORTKEY_VIRTUAL_KEY_OPENAI}"
    anthropic: "${PORTKEY_VIRTUAL_KEY_ANTHROPIC}"
    gemini: "${PORTKEY_VIRTUAL_KEY_GEMINI}"
    openrouter: "${PORTKEY_VIRTUAL_KEY_OPENROUTER}"
  default_models:
    primary: "openai/gpt-4o"
    fallback_openai: "gpt-4o-mini"
    fallback_anthropic: "claude-3-haiku-20240307"
    fallback_gemini: "gemini-1.5-pro"
  security:
    pii_redaction: true # Redact PII in prompts/responses
    enable_guardrails: true # Enable safety guardrails
    log_prompt_level: "none" # Options: none, metadata, full
    log_response_level: "none" # Options: none, metadata, full

# Kong Configuration (Fallback)
kong:
  admin_url: "${KONG_ADMIN_URL}"
  gateway_url: "${KONG_GATEWAY_URL}"
  auth_token: "${KONG_AUTH_TOKEN}"
  plugins:
    - name: "circuit-breaker"
      config:
        timeout: 30
        threshold: 10
    - name: "rate-limiting"
      config:
        second: 10
        policy: "local"
    - name: "request-transformer"
      config:
        enable_pii_masking: true

# Dynamic Routing Rules
routing_rules:
  - name: "code-generation"
    pattern: "code|function|script|programming"
    provider: "openai"
    model: "gpt-4o"

  - name: "business-analytics"
    pattern: "sales|revenue|forecast|business|analytics"
    provider: "anthropic"
    model: "claude-3-sonnet-20240229"

  - name: "creative-content"
    pattern: "story|creative|poem|imagine"
    provider: "anthropic"
    model: "claude-3-opus-20240229"

  - name: "research-complex"
    pattern: "research|analyze|investigate"
    provider: "openai"
    model: "gpt-4o"

  - name: "quick-response"
    pattern: ".*" # Default fallback pattern
    provider: "openai"
    model: "gpt-4o-mini"

# Agent Orchestration Integration
agent_orchestration:
  # Map agent types to specific models
  agent_model_mapping:
    researcher: "anthropic/claude-3-opus-20240229"
    executor: "openai/gpt-4o"
    validator: "anthropic/claude-3-sonnet-20240229"
    summarizer: "openai/gpt-4o-mini"
    coder: "openai/gpt-4o"

  # Configure how agent context gets passed between models
  context_management:
    max_context_tokens: 2000000 # Gemini's 2M context window
    context_strategy: "priority_based"
    priority_threshold: 0.7
    enable_summarization: true
    summarization_model: "anthropic/claude-3-haiku-20240307"

# Cost Management
cost_management:
  enable_budget_limits: true
  monthly_budget_usd: 1000
  alert_threshold_percent: 80
  cost_optimization_strategy: "balanced" # Options: cost, performance, balanced
  token_budget_per_request: 8000

# Observability
observability:
  log_level: "info"
  enable_tracing: true
  trace_sampling_ratio: 0.1
  export_metrics: true
  metrics_endpoint: "${METRICS_ENDPOINT}"
  enable_cost_tracking: true
