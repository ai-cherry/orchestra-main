# LiteLLM Configuration for Orchestra
# This configuration sets up model providers, routing, caching, monitoring, and logging
# Last updated: 2025-05-23
#
# Configuration sections:
# - General settings: Basic operational parameters
# - Model list: Available models with provider-specific configurations
# - Router settings: Load balancing and fallback strategies
# - Cache settings: Response caching for cost optimization
# - Monitoring: Performance and usage tracking
# - Cost optimization: Budget controls and efficiency settings
# - Logging: Debugging and analytics
# - Security: API key management and access controls

# General settings
general_settings:
  # Toggle for verbose logging (overridable via environment variable)
  verbose: false # Set to true for detailed logs
  # Maximum number of retries for failed API calls
  num_retries: 3
  # Context window management
  context_window_fallbacks: true
  # Set to false in production to prevent exposing full error messages
  debug: false
  # Request timeout (seconds)
  request_timeout: 30
  # Stream timeout for streaming responses (seconds)
  stream_timeout: 60

# Model provider configurations
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: azure/gpt-35-turbo
      api_base: ${AZURE_API_BASE}
      api_key: ${AZURE_API_KEY}
      api_version: "2023-07-01-preview"

  - model_name: gpt-4
    litellm_params:
      model: azure/gpt-4
      api_base: ${AZURE_API_BASE}
      api_key: ${AZURE_API_KEY}
      api_version: "2023-07-01-preview"

  # Claude 3 models with MCP support
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: ${ANTHROPIC_API_KEY}
      # Enable MCP connector support for Claude 3
      extra_headers:
        anthropic-beta: "mcp-client-2025-04-04"
      # Cost optimization: Set max tokens to prevent runaway costs
      max_tokens: 4096

  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: ${ANTHROPIC_API_KEY}
      extra_headers:
        anthropic-beta: "mcp-client-2025-04-04"
      max_tokens: 4096

  # Portkey integration with proper headers
  - model_name: portkey/gpt-4o
    litellm_params:
      model: openai/portkey/gpt-4o
      api_base: "https://api.portkey.ai/v1"
      api_key: ${PORTKEY_API_KEY}
      extra_headers:
        x-portkey-provider: "openai"
        x-portkey-config: "${PORTKEY_CONFIG}"

  - model_name: portkey/claude-3-opus
    litellm_params:
      model: anthropic/portkey/claude-3-opus
      api_base: "https://api.portkey.ai/v1"
      api_key: ${PORTKEY_API_KEY}
      extra_headers:
        x-portkey-provider: "anthropic"
        x-portkey-config: "${PORTKEY_CONFIG}"

  # OpenRouter integration with full model paths
  - model_name: openrouter/mixtral
    litellm_params:
      model: openrouter/mistralai/mixtral-8x7b-instruct
      api_base: "https://openrouter.ai/api/v1"
      api_key: ${OPENROUTER_API_KEY}
      # OpenRouter requires HTTP-Referer - set via environment variables
      # OR_SITE_URL and OR_APP_NAME

  # Google AI / Gemini Models with Vertex AI compatibility
  - model_name: llm_gemini_pro
    litellm_params:
      model: vertex_ai/gemini-1.5-pro-preview-0514
      vertex_project: ${VULTR_PROJECT_ID}
      vertex_location: us-central1
      temperature: 0.7
      max_tokens: 2048

  - model_name: llm_gemini_flash
    litellm_params:
      model: vertex_ai/gemini-1.5-flash-preview-0514
      vertex_project: ${VULTR_PROJECT_ID}
      vertex_location: us-central1
      temperature: 0.7
      max_tokens: 2048

  - model_name: llm_gemini_ultra
    litellm_params:
      model: vertex_ai/gemini-ultra
      vertex_project: ${VULTR_PROJECT_ID}
      vertex_location: us-central1
      temperature: 0.7
      max_tokens: 4096

# Model routing configuration based on parameters
router_settings:
  # Set to "simple" for round-robin, "shortest-queue" for load-based, "latency-based" for performance
  routing_strategy: "latency-based" # Optimize for performance

  # Timeout settings in seconds
  timeout: 30

  # Fallback models if primary fails
  fallbacks:
    [
      { "model": "gpt-4", "fallback_model": "claude-3-opus" },
      { "model": "claude-3-opus", "fallback_model": "gpt-4" },
      { "model": "claude-3-sonnet", "fallback_model": "gpt-3.5-turbo" },
    ]

  # Model selection based on task complexity
  model_selection:
    # Use economy models for simple tasks
    simple_tasks: ["gpt-3.5-turbo"]
    # Use standard models for general purpose
    general_tasks: ["claude-3-sonnet"]
    # Use premium models for complex reasoning
    complex_tasks: ["gpt-4", "claude-3-opus"]

# Cache configuration
cache_settings:
  # Cache type: "redis" or "in-memory"
  cache_type: "redis"
  # Cache parameters for Redis
  redis_host: ${REDIS_HOST}
  redis_port: ${REDIS_PORT}
  redis_password: ${REDIS_PASSWORD}
  # Time to live for cached responses (seconds)
  ttl: 3600
  # Semantic caching for intelligent response reuse
  semantic_cache: true
  # Embedding model for semantic cache
  embedding_model: "text-embedding-ada-002"
  # Cache similarity threshold (0.0 to 1.0)
  similarity_threshold: 0.95

# Monitoring configuration
monitoring:
  # Enable comprehensive monitoring
  enabled: true
  # Metrics to track
  metrics:
    - latency # Response time tracking
    - token_usage # Token consumption
    - cost_per_request # Cost tracking
    - error_rate # Error tracking
    - cache_hit_rate # Cache effectiveness
    - model_performance # Model-specific metrics

  # Export metrics to monitoring systems
  exporters:
    # Prometheus metrics endpoint
    prometheus:
      enabled: true
      port: 9090
      path: "/metrics"

    # Vultr Monitoring
    cloud_monitoring:
      enabled: true
      project_id: ${Vultr_PROJECT_ID}

    # Custom webhooks for alerts
    webhooks:
      - url: ${ALERT_WEBHOOK_URL}
        events: ["error", "budget_alert", "rate_limit"]

  # Performance tracking
  performance:
    # Track p50, p90, p99 latencies
    percentiles: [0.5, 0.9, 0.99]
    # Track slow requests (ms)
    slow_request_threshold: 5000

# Logging configuration
logging:
  # Log to Cloud Logging in Vultr environments
  cloud_logging: true
  # Log level
  level: "INFO" # DEBUG, INFO, WARNING, ERROR (use INFO in production)
  # Request & response logging for analytics
  log_prompts: true
  log_responses: false # Disable in production for privacy
  # Structured logging format
  format: "json"
  # Log sampling for high volume
  sampling_rate: 0.1 # Log 10% of requests in production

# Cost optimization settings
cost_optimization:
  # Enable cost optimization features
  enabled: true

  # Automatic model downgrade based on task
  auto_downgrade:
    enabled: true
    # Downgrade rules based on prompt characteristics
    rules:
      - condition: "prompt_length < 100"
        target_tier: "economy"
      - condition: "no_code_generation"
        target_tier: "standard"

  # Token optimization
  token_optimization:
    # Compress system prompts
    compress_system_prompts: true
    # Remove redundant whitespace
    trim_whitespace: true
    # Truncate verbose responses
    max_response_tokens: 2048

  # Request batching for efficiency
  batching:
    enabled: true
    max_batch_size: 10
    max_wait_time: 100 # ms

# Cost tracking & budget controls
budget_manager:
  # Enable budget tracking
  is_enabled: true
  # Project identifiers
  project_id: "orchestra"
  # Budget in USD
  max_budget: 100.0
  # Per-model budgets
  model_budgets:
    "claude-3-opus": 30.0
    "claude-3-sonnet": 20.0
    "gpt-4": 25.0
    "gpt-3.5-turbo": 10.0
  # Budget reached callback
  budget_alerts: ["50%", "80%", "90%", "100%"]
  # Actions when budget exceeded
  budget_exceeded_action: "fallback_to_cheaper" # or "block"

# Security settings
security:
  # API key hashing for logs
  hash_keys: true
  # Access controls
  use_key_management: true
  master_key: ${LITELLM_MASTER_KEY}
  # Rate limiting per API key
  rate_limiting:
    enabled: true
    default_rpm: 60 # Requests per minute
    default_tpm: 100000 # Tokens per minute
  # IP allowlist (optional)
  ip_allowlist: [] # Add allowed IPs if needed

# Custom headers for provider API requests
extra_headers:
  user_id: "${USER_ID}"
  session_id: "${SESSION_ID}"
  # Add request ID for tracing
  x-request-id: "${REQUEST_ID}"
  # Add MCP context when available
  x-mcp-context: "${MCP_CONTEXT}"
