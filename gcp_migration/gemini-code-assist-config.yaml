# Performance-optimized Gemini Code Assist configuration for AI Orchestra
# This configuration prioritizes performance, stability, and deterministic responses

model:
  # Use the latest version of Gemini Pro for optimal code generation
  name: gemini-pro-latest
  
  # Temperature: 0.2 (lower than default)
  # This lower temperature setting produces more deterministic, consistent code
  # outputs, reducing hallucinations and improving code quality.
  temperature: 0.2
  
  # Increased token limit for handling complex code generation tasks
  max_output_tokens: 8192
  
  # Controls diversity of output (slightly lower for more focus)
  top_p: 0.95

# Tool integrations for external APIs and services
tool_integrations:
  # Connect to Vertex AI for enhanced capabilities
  vertex_ai:
    endpoint: projects/cherry-ai-project/locations/us-central1/endpoints/agent-core
    api_version: v1
  
  # Redis integration for fast memory access
  redis:
    connection_string: redis://vertex-agent@cherry-ai-project
  
  # Database connection for persistent storage
  database:
    connection_string: postgresql://alloydb-user@alloydb-instance:5432/cherry_ai_project

# Performance-first project priorities
priorities:
  # Primary focus areas
  focus: 
    - performance
    - stability
    - optimization
  
  # Secondary considerations
  secondary:
    - basic_security
    - accessibility
  
  # Configuration to inform assistant about project philosophy
  instructions: |
    This project follows a "performance-first" approach where:
    1. Performance and stability are the primary concerns
    2. Code should be optimized for vector operations and memory efficiency
    3. Use async/await patterns for IO-bound operations
    4. Leverage GCP services where appropriate for scaling
    5. Implement circuit breaker patterns for resilience
    6. Use Pydantic models for validation
    7. Follow Python 3.11+ best practices
    8. Use type hints throughout the codebase

# IDE integration settings
ide_integration:
  # Enable real-time suggestions
  suggestions_enabled: true
  
  # Prioritize performance in suggestions
  suggestion_mode: performance
  
  # Enable code context from open files
  context_from_open_files: true
  
  # Increase context window size
  context_size: maximum