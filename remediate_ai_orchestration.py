#!/usr/bin/env python3
"""
Comprehensive AI Orchestration Remediation Script
Systematically addresses all identified issues with detailed tracking
"""

import os
import sys
import json
import subprocess
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Any
import shutil
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('remediation_log.txt'),
        logging.StreamHandler()
    ]
)

class AIOrchestrationRemediator:
    def __init__(self):
        self.issues_fixed = []
        self.issues_pending = []
        self.test_results = {}
        self.start_time = datetime.now()
        
    def log_action(self, action: str, status: str, details: str = ""):
        """Log remediation actions with status tracking"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "status": status,
            "details": details
        }
        
        if status == "SUCCESS":
            self.issues_fixed.append(log_entry)
            logging.info(f"‚úÖ {action}: {details}")
        elif status == "FAILED":
            self.issues_pending.append(log_entry)
            logging.error(f"‚ùå {action}: {details}")
        else:
            logging.warning(f"‚ö†Ô∏è  {action}: {details}")
            
    def fix_core_monitoring_init(self):
        """Fix the indentation error in core/monitoring/__init__.py"""
        try:
            init_path = "core/monitoring/__init__.py"
            
            # Read current content
            with open(init_path, 'r') as f:
                content = f.read()
            
            # Fix the indentation issue
            fixed_content = '''"""Core monitoring module for the Orchestra system."""

from .metrics_collector import MetricsCollector
from .health_checker import HealthChecker
from .performance_monitor import PerformanceMonitor

__all__ = [
    'MetricsCollector',
    'HealthChecker', 
    'PerformanceMonitor'
]
'''
            
            # Write fixed content
            with open(init_path, 'w') as f:
                f.write(fixed_content)
                
            self.log_action("Fix core.monitoring __init__.py", "SUCCESS", 
                          "Fixed indentation error")
            return True
            
        except Exception as e:
            self.log_action("Fix core.monitoring __init__.py", "FAILED", str(e))
            return False
            
    def install_dependencies(self):
        """Install all missing dependencies"""
        try:
            # First, ensure pip is up to date
            subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "pip"], 
                         check=True, capture_output=True)
            
            # Install from requirements file
            result = subprocess.run(
                [sys.executable, "-m", "pip", "install", "-r", "requirements_ai_orchestration.txt"],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                self.log_action("Install dependencies", "SUCCESS", 
                              "All dependencies installed successfully")
                return True
            else:
                self.log_action("Install dependencies", "FAILED", 
                              f"Error: {result.stderr}")
                return False
                
        except Exception as e:
            self.log_action("Install dependencies", "FAILED", str(e))
            return False
            
    def create_environment_config(self):
        """Create environment configuration with secure defaults"""
        try:
            env_content = '''# AI Orchestration Environment Configuration
# Copy to .env and update with your actual values

# Database Configuration
DATABASE_URL=postgresql://orchestra_user:secure_password@localhost:5432/orchestra_db
REDIS_URL=redis://localhost:6379/0
WEAVIATE_URL=http://localhost:8080

# API Keys (obtain from respective services)
OPENAI_API_KEY=your_openai_api_key_here
GITHUB_TOKEN=your_github_token_here
SLACK_BOT_TOKEN=your_slack_bot_token_here
HUBSPOT_API_KEY=your_hubspot_api_key_here
GONG_API_KEY=your_gong_api_key_here

# Security Configuration
SECRET_KEY=generate_a_secure_random_key_here
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000
JWT_SECRET_KEY=another_secure_random_key_here

# Performance Configuration
MAX_CONCURRENT_AGENTS=50
AGENT_MEMORY_LIMIT_MB=512
REQUEST_TIMEOUT_SECONDS=60
CONNECTION_POOL_SIZE=20

# Monitoring Configuration
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_LEVEL=INFO
SENTRY_DSN=your_sentry_dsn_here

# Feature Flags
ENABLE_WEB_SCRAPING=true
ENABLE_AI_OPERATORS=true
ENABLE_CIRCUIT_BREAKERS=true
'''
            
            with open('.env.example', 'w') as f:
                f.write(env_content)
                
            self.log_action("Create environment config", "SUCCESS", 
                          "Created .env.example with secure defaults")
            return True
            
        except Exception as e:
            self.log_action("Create environment config", "FAILED", str(e))
            return False
            
    def add_input_validation(self):
        """Add input validation middleware"""
        try:
            validation_code = '''"""
Input validation middleware for AI Orchestration
Prevents injection attacks and validates all inputs
"""

import re
import json
from typing import Any, Dict, List, Optional
from functools import wraps
import bleach
from pydantic import BaseModel, validator, ValidationError

class InputValidator:
    """Comprehensive input validation for all user inputs"""
    
    # SQL injection patterns
    SQL_INJECTION_PATTERNS = [
        r"(\b(SELECT|INSERT|UPDATE|DELETE|DROP|UNION|CREATE|ALTER)\b)",
        r"(--|#|/\*|\*/)",
        r"(\bOR\b\s*\d+\s*=\s*\d+)",
        r"(\bAND\b\s*\d+\s*=\s*\d+)",
        r"(;|\||&&)"
    ]
    
    # XSS patterns
    XSS_PATTERNS = [
        r"<script[^>]*>.*?</script>",
        r"javascript:",
        r"on\w+\s*=",
        r"<iframe[^>]*>",
        r"<object[^>]*>"
    ]
    
    @classmethod
    def sanitize_string(cls, value: str, max_length: int = 1000) -> str:
        """Sanitize string input"""
        if not isinstance(value, str):
            raise ValueError("Input must be a string")
            
        # Truncate to max length
        value = value[:max_length]
        
        # Remove any HTML tags
        value = bleach.clean(value, tags=[], strip=True)
        
        # Check for SQL injection patterns
        for pattern in cls.SQL_INJECTION_PATTERNS:
            if re.search(pattern, value, re.IGNORECASE):
                raise ValueError("Potential SQL injection detected")
                
        # Check for XSS patterns
        for pattern in cls.XSS_PATTERNS:
            if re.search(pattern, value, re.IGNORECASE):
                raise ValueError("Potential XSS attack detected")
                
        return value
        
    @classmethod
    def validate_json(cls, value: str) -> Dict[str, Any]:
        """Validate JSON input"""
        try:
            data = json.loads(value)
            # Recursively sanitize all string values
            return cls._sanitize_dict(data)
        except json.JSONDecodeError:
            raise ValueError("Invalid JSON format")
            
    @classmethod
    def _sanitize_dict(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Recursively sanitize dictionary values"""
        sanitized = {}
        for key, value in data.items():
            if isinstance(value, str):
                sanitized[key] = cls.sanitize_string(value)
            elif isinstance(value, dict):
                sanitized[key] = cls._sanitize_dict(value)
            elif isinstance(value, list):
                sanitized[key] = [
                    cls._sanitize_dict(item) if isinstance(item, dict)
                    else cls.sanitize_string(item) if isinstance(item, str)
                    else item
                    for item in value
                ]
            else:
                sanitized[key] = value
        return sanitized

# Pydantic models for request validation
class QueryRequest(BaseModel):
    query: str
    domain: Optional[str] = None
    max_results: int = 10
    
    @validator('query')
    def validate_query(cls, v):
        return InputValidator.sanitize_string(v, max_length=500)
        
    @validator('domain')
    def validate_domain(cls, v):
        if v and v not in ['cherry', 'sophia', 'paragonrx']:
            raise ValueError('Invalid domain')
        return v
        
    @validator('max_results')
    def validate_max_results(cls, v):
        if v < 1 or v > 100:
            raise ValueError('max_results must be between 1 and 100')
        return v

class WebScrapingRequest(BaseModel):
    url: str
    scraping_type: str
    
    @validator('url')
    def validate_url(cls, v):
        # Basic URL validation
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        
        if not url_pattern.match(v):
            raise ValueError('Invalid URL format')
        return v

def validate_input(request_model: BaseModel):
    """Decorator for input validation"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Extract request data from kwargs or args
            request_data = kwargs.get('request_data') or (args[1] if len(args) > 1 else {})
            
            try:
                # Validate using Pydantic model
                validated_data = request_model(**request_data)
                kwargs['validated_data'] = validated_data
                return await func(*args, **kwargs)
            except ValidationError as e:
                raise ValueError(f"Input validation failed: {e}")
                
        return wrapper
    return decorator
'''
            
            with open('core/security/input_validation.py', 'w') as f:
                f.write(validation_code)
                
            # Create __init__.py for security module
            os.makedirs('core/security', exist_ok=True)
            with open('core/security/__init__.py', 'w') as f:
                f.write('from .input_validation import InputValidator, validate_input, QueryRequest, WebScrapingRequest\n')
                
            self.log_action("Add input validation", "SUCCESS", 
                          "Created comprehensive input validation middleware")
            return True
            
        except Exception as e:
            self.log_action("Add input validation", "FAILED", str(e))
            return False
            
    def configure_cors(self):
        """Configure CORS for web access"""
        try:
            cors_config = '''"""
CORS configuration for AI Orchestration API
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import os

def configure_cors(app: FastAPI):
    """Configure CORS middleware with security best practices"""
    
    # Get allowed origins from environment
    allowed_origins = os.getenv(
        "CORS_ALLOWED_ORIGINS", 
        "http://localhost:3000,http://localhost:8000"
    ).split(",")
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        allow_headers=[
            "Content-Type",
            "Authorization",
            "X-Requested-With",
            "X-CSRF-Token"
        ],
        expose_headers=["X-Total-Count", "X-Page-Count"],
        max_age=3600  # Cache preflight requests for 1 hour
    )
    
    # Add security headers middleware
    @app.middleware("http")
    async def add_security_headers(request, call_next):
        response = await call_next(request)
        
        # Security headers
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        response.headers["Content-Security-Policy"] = "default-src 'self'"
        
        return response
        
    return app
'''
            
            with open('core/security/cors_config.py', 'w') as f:
                f.write(cors_config)
                
            self.log_action("Configure CORS", "SUCCESS", 
                          "Created CORS configuration with security headers")
            return True
            
        except Exception as e:
            self.log_action("Configure CORS", "FAILED", str(e))
            return False
            
    def add_resource_limits(self):
        """Add resource limits and overflow handling"""
        try:
            resource_mgmt = '''"""
Resource management and limits for AI agents
Prevents resource exhaustion and handles overflow
"""

import asyncio
import psutil
import os
from typing import Dict, Optional, Any
from datetime import datetime
import logging
from collections import deque
import resource

logger = logging.getLogger(__name__)

class ResourceManager:
    """Manages resource allocation and limits for AI agents"""
    
    def __init__(self):
        self.max_concurrent_agents = int(os.getenv("MAX_CONCURRENT_AGENTS", "50"))
        self.agent_memory_limit_mb = int(os.getenv("AGENT_MEMORY_LIMIT_MB", "512"))
        self.active_agents = {}
        self.agent_queue = asyncio.Queue(maxsize=200)  # Overflow queue
        self.metrics = {
            "total_agents_created": 0,
            "agents_rejected": 0,
            "memory_limit_hits": 0,
            "queue_overflows": 0
        }
        
    async def acquire_agent_slot(self, agent_id: str) -> bool:
        """Try to acquire a slot for a new agent"""
        if len(self.active_agents) >= self.max_concurrent_agents:
            # Try to queue the agent
            try:
                await asyncio.wait_for(
                    self.agent_queue.put(agent_id),
                    timeout=5.0
                )
                logger.warning(f"Agent {agent_id} queued due to concurrency limit")
                self.metrics["queue_overflows"] += 1
                return False
            except asyncio.TimeoutError:
                logger.error(f"Agent {agent_id} rejected - queue full")
                self.metrics["agents_rejected"] += 1
                raise RuntimeError("Agent queue full - system overloaded")
                
        self.active_agents[agent_id] = {
            "start_time": datetime.now(),
            "memory_usage": 0
        }
        self.metrics["total_agents_created"] += 1
        return True
        
    def release_agent_slot(self, agent_id: str):
        """Release an agent slot"""
        if agent_id in self.active_agents:
            del self.active_agents[agent_id]
            
        # Check if any queued agents can now run
        if not self.agent_queue.empty():
            asyncio.create_task(self._process_queue())
            
    async def _process_queue(self):
        """Process queued agents"""
        while not self.agent_queue.empty() and len(self.active_agents) < self.max_concurrent_agents:
            try:
                agent_id = await self.agent_queue.get()
                await self.acquire_agent_slot(agent_id)
            except Exception as e:
                logger.error(f"Error processing agent queue: {e}")
                
    def check_memory_limit(self, agent_id: str) -> bool:
        """Check if agent is within memory limits"""
        try:
            process = psutil.Process(os.getpid())
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            if agent_id in self.active_agents:
                self.active_agents[agent_id]["memory_usage"] = memory_mb
                
            if memory_mb > self.agent_memory_limit_mb:
                logger.error(f"Agent {agent_id} exceeded memory limit: {memory_mb}MB")
                self.metrics["memory_limit_hits"] += 1
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"Error checking memory limit: {e}")
            return True
            
    def get_system_health(self) -> Dict[str, Any]:
        """Get current system health metrics"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        return {
            "active_agents": len(self.active_agents),
            "queued_agents": self.agent_queue.qsize(),
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "metrics": self.metrics,
            "health_status": self._calculate_health_status(cpu_percent, memory.percent)
        }
        
    def _calculate_health_status(self, cpu: float, memory: float) -> str:
        """Calculate overall system health"""
        if cpu > 90 or memory > 90:
            return "CRITICAL"
        elif cpu > 70 or memory > 70:
            return "WARNING"
        else:
            return "HEALTHY"

# Global resource manager instance
resource_manager = ResourceManager()

class ResourceLimitedAgent:
    """Base class for resource-limited agents"""
    
    def __init__(self, agent_id: str):
        self.agent_id = agent_id
        self.resource_manager = resource_manager
        
    async def __aenter__(self):
        """Acquire resources on entry"""
        success = await self.resource_manager.acquire_agent_slot(self.agent_id)
        if not success:
            raise RuntimeError("Unable to acquire agent slot")
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Release resources on exit"""
        self.resource_manager.release_agent_slot(self.agent_id)
        
    async def execute_with_limits(self, func, *args, **kwargs):
        """Execute function with resource limits"""
        # Check memory before execution
        if not self.resource_manager.check_memory_limit(self.agent_id):
            raise MemoryError("Agent memory limit exceeded")
            
        # Set process limits
        soft, hard = resource.getrlimit(resource.RLIMIT_AS)
        resource.setrlimit(resource.RLIMIT_AS, 
                         (self.resource_manager.agent_memory_limit_mb * 1024 * 1024, hard))
        
        try:
            return await func(*args, **kwargs)
        finally:
            # Reset limits
            resource.setrlimit(resource.RLIMIT_AS, (soft, hard))
'''
            
            with open('core/resource_management.py', 'w') as f:
                f.write(resource_mgmt)
                
            self.log_action("Add resource limits", "SUCCESS", 
                          "Created resource management with overflow handling")
            return True
            
        except Exception as e:
            self.log_action("Add resource limits", "FAILED", str(e))
            return False
            
    def optimize_performance(self):
        """Optimize performance configurations"""
        try:
            perf_config = '''"""
Performance optimization configurations
"""

import os
from typing import Dict, Any

class PerformanceConfig:
    """Performance tuning configurations"""
    
    # Connection pool settings
    DATABASE_POOL_SIZE = int(os.getenv("CONNECTION_POOL_SIZE", "20"))
    DATABASE_POOL_TIMEOUT = 30
    DATABASE_POOL_RECYCLE = 3600
    
    # Request timeouts
    REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT_SECONDS", "60"))
    AGENT_TASK_TIMEOUT = 120
    WEB_SCRAPING_TIMEOUT = 30
    
    # Caching settings
    CACHE_TTL = 300  # 5 minutes
    CACHE_MAX_SIZE = 1000
    
    # Batch processing
    BATCH_SIZE = 50
    MAX_BATCH_WAIT = 1.0  # seconds
    
    # Circuit breaker settings
    CIRCUIT_BREAKER_FAILURE_THRESHOLD = 5
    CIRCUIT_BREAKER_RECOVERY_TIMEOUT = 60
    CIRCUIT_BREAKER_EXPECTED_EXCEPTION = (ConnectionError, TimeoutError)
    
    @classmethod
    def get_database_config(cls) -> Dict[str, Any]:
        """Get optimized database configuration"""
        return {
            "pool_size": cls.DATABASE_POOL_SIZE,
            "max_overflow": 10,
            "pool_timeout": cls.DATABASE_POOL_TIMEOUT,
            "pool_recycle": cls.DATABASE_POOL_RECYCLE,
            "pool_pre_ping": True,
            "echo": False,
            "connect_args": {
                "connect_timeout": 10,
                "application_name": "ai_orchestration",
                "options": "-c statement_timeout=60000"  # 60 second statement timeout
            }
        }
        
    @classmethod
    def get_redis_config(cls) -> Dict[str, Any]:
        """Get optimized Redis configuration"""
        return {
            "decode_responses": True,
            "max_connections": 50,
            "socket_timeout": 5,
            "socket_connect_timeout": 5,
            "retry_on_timeout": True,
            "health_check_interval": 30
        }
        
    @classmethod
    def get_async_config(cls) -> Dict[str, Any]:
        """Get async execution configuration"""
        return {
            "max_workers": os.cpu_count() * 2,
            "task_timeout": cls.AGENT_TASK_TIMEOUT,
            "graceful_shutdown_timeout": 30
        }

# Query optimization helpers
OPTIMIZED_INDEXES = [
    # Agent queries
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_agents_status ON agents(status) WHERE status = 'active'",
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_agents_domain ON agents(domain)",
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_agents_created ON agents(created_at DESC)",
    
    # Task queries
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tasks_agent_status ON tasks(agent_id, status)",
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tasks_priority ON tasks(priority DESC, created_at)",
    
    # Metrics queries
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_metrics_timestamp ON metrics(timestamp DESC)",
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_metrics_agent_time ON metrics(agent_id, timestamp DESC)",
    
    # Composite indexes for common queries
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_agents_domain_status ON agents(domain, status)",
    "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tasks_status_priority ON tasks(status, priority DESC)"
]

# Materialized views for reporting
MATERIALIZED_VIEWS = [
    """
    CREATE MATERIALIZED VIEW IF NOT EXISTS agent_performance_summary AS
    SELECT 
        a.id as agent_id,
        a.domain,
        COUNT(t.id) as total_tasks,
        AVG(t.execution_time) as avg_execution_time,
        SUM(CASE WHEN t.status = 'completed' THEN 1 ELSE 0 END) as completed_tasks,
        SUM(CASE WHEN t.status = 'failed' THEN 1 ELSE 0 END) as failed_tasks,
        MAX(t.completed_at) as last_activity
    FROM agents a
    LEFT JOIN tasks t ON a.id = t.agent_id
    GROUP BY a.id, a.domain
    """,
    
    """
    CREATE MATERIALIZED VIEW IF NOT EXISTS hourly_metrics AS
    SELECT 
        date_trunc('hour', timestamp) as hour,
        agent_id,
        AVG(response_time) as avg_response_time,
        COUNT(*) as request_count,
        SUM(CASE WHEN error_code IS NOT NULL THEN 1 ELSE 0 END) as error_count
    FROM metrics
    WHERE timestamp > NOW() - INTERVAL '7 days'
    GROUP BY date_trunc('hour', timestamp), agent_id
    """
]
'''
            
            with open('core/performance_config.py', 'w') as f:
                f.write(perf_config)
                
            self.log_action("Optimize performance", "SUCCESS", 
                          "Created performance optimization configurations")
            return True
            
        except Exception as e:
            self.log_action("Optimize performance", "FAILED", str(e))
            return False
            
    def add_monitoring_alerting(self):
        """Add comprehensive monitoring and alerting"""
        try:
            monitoring_code = '''"""
Enhanced monitoring and alerting system
"""

import os
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import aiohttp
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import json

logger = logging.getLogger(__name__)

# Prometheus metrics
request_count = Counter('ai_orchestration_requests_total',
                       'Total requests', ['method', 'endpoint', 'status'])
request_duration = Histogram('ai_orchestration_request_duration_seconds',
                           'Request duration', ['method', 'endpoint'])
active_agents_gauge = Gauge('ai_orchestration_active_agents',
                          'Number of active agents', ['domain'])
error_count = Counter('ai_orchestration_errors_total',
                     'Total errors', ['error_type', 'component'])
queue_size = Gauge('ai_orchestration_queue_size',
                  'Agent queue size')

class AlertManager:
    """Manages alerts and notifications"""
    
    def __init__(self):
        self.alert_thresholds = {
            "error_rate": 0.05,  # 5% error rate
            "response_time_p95": 2.0,  # 2 seconds
            "cpu_usage": 80,  # 80%
            "memory_usage": 85,  # 85%
            "queue_size": 100
        }
        self.alert_history = []
        self.webhook_url = os.getenv("ALERT_WEBHOOK_URL")
        
    async def check_alerts(self, metrics: Dict[str, Any]):
        """Check metrics against thresholds and send alerts"""
        alerts = []
        
        # Check error rate
        if metrics.get("error_rate", 0) > self.alert_thresholds["error_rate"]:
            alerts.append({
                "severity": "HIGH",
                "type": "ERROR_RATE",
                "message": f"Error rate {metrics['error_rate']:.2%} exceeds threshold",
                "value": metrics["error_rate"]
            })
            
        # Check response time
        if metrics.get("response_time_p95", 0) > self.alert_thresholds["response_time_p95"]:
            alerts.append({
                "severity": "MEDIUM",
                "type": "RESPONSE_TIME",
                "message": f"P95 response time {metrics['response_time_p95']:.2f}s exceeds threshold",
                "value": metrics["response_time_p95"]
            })
            
        # Check resource usage
        if metrics.get("cpu_usage", 0) > self.alert_thresholds["cpu_usage"]:
            alerts.append({
                "severity": "HIGH",
                "type": "CPU_USAGE",
                "message": f"CPU usage {metrics['cpu_usage']:.1f}% exceeds threshold",
                "value": metrics["cpu_usage"]
            })
            
        # Send alerts
        for alert in alerts:
            await self.send_alert(alert)
            
    async def send_alert(self, alert: Dict[str, Any]):
        """Send alert via webhook"""
        alert["timestamp"] = datetime.now().isoformat()
        self.alert_history.append(alert)
        
        if self.webhook_url:
            try:
                async with aiohttp.ClientSession() as session:
                    await session.post(self.webhook_url, json=alert)
                logger.info(f"Alert sent: {alert['type']} - {alert['message']}")
            except Exception as e:
                logger.error(f"Failed to send alert: {e}")
        else:
            logger.warning(f"Alert (no webhook configured): {alert}")

class MetricsCollector:
    """Collects and aggregates system metrics"""
    
    def __init__(self):
        self.metrics_buffer = []
        self.collection_interval = 60  # seconds
        
    async def collect_metrics(self) -> Dict[str, Any]:
        """Collect current system metrics"""
        from core.resource_management import resource_manager
        
        # Get system health
        health = resource_manager.get_system_health()
        
        # Calculate error rate from recent requests
        error_rate = self._calculate_error_rate()
        
        # Get response time percentiles
        response_times = self._get_response_time_percentiles()
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "active_agents": health["active_agents"],
            "queued_agents": health["queued_agents"],
            "cpu_usage": health["cpu_percent"],
            "memory_usage": health["memory_percent"],
            "error_rate": error_rate,
            "response_time_p50": response_times.get("p50", 0),
            "response_time_p95": response_times.get("p95", 0),
            "response_time_p99": response_times.get("p99", 0),
            "health_status": health["health_status"]
        }
        
        # Update Prometheus metrics
        queue_size.set(health["queued_agents"])
        
        return metrics
        
    def _calculate_error_rate(self) -> float:
        """Calculate error rate from recent requests"""
        # This would query actual metrics from database
        # Placeholder implementation
        return 0.02  # 2% error rate
        
    def _get_response_time_percentiles(self) -> Dict[str, float]:
        """Get response time percentiles"""
        # Placeholder implementation
        return {
            "p50": 0.5,
            "p95": 1.5,
            "p99": 2.5
        }

# Global instances
alert_manager = AlertManager()
metrics_collector = MetricsCollector()

async def start_monitoring():
    """Start the monitoring loop"""
    while True:
        try:
            metrics = await metrics_collector.collect_metrics()
            await alert_manager.check_alerts(metrics)
            await asyncio.sleep(metrics_collector.collection_interval)
        except Exception as e:
            logger.error(f"Monitoring error: {e}")
            await asyncio.sleep(60)
'''
            
            with open('core/monitoring/enhanced_monitoring.py', 'w') as f:
                f.write(monitoring_code)
                
            self.log_action("Add monitoring/alerting", "SUCCESS",
                          "Created enhanced monitoring with alerting")
            return True
            
        except Exception as e:
            self.log_action("Add monitoring/alerting", "FAILED", str(e))
            return False
            
    def create_integration_tests(self):
        """Create comprehensive integration tests"""
        try:
            test_code = '''"""
Integration tests for AI Orchestration system
"""

import pytest
import asyncio
import os
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
import json

# Set test environment
os.environ["TESTING"] = "true"
os.environ["DATABASE_URL"] = "postgresql://test_user:test_pass@localhost:5432/test_db"
os.environ["REDIS_URL"] = "redis://localhost:6379/1"
os.environ["WEAVIATE_URL"] = "http://localhost:8080"

@pytest.fixture
async def orchestrator():
    """Create test orchestrator instance"""
    from services.ai_agent_orchestrator import AIAgentOrchestrator
    
    # Mock external dependencies
    with patch('services.ai_agent_orchestrator.create_async_engine') as mock_engine:
        mock_engine.return_value = AsyncMock()
        orchestrator = AIAgentOrchestrator()
        await orchestrator.initialize()
        yield orchestrator
        await orchestrator.shutdown()

@pytest.fixture
def mock_agents():
    """Mock agent instances"""
    agents = {
        "cherry": Mock(domain="cherry", process_query=AsyncMock()),
        "sophia": Mock(domain="sophia", process_query=AsyncMock()),
        "paragonrx": Mock(domain="paragonrx", process_query=AsyncMock())
    }
    return agents

class TestAIOrchestration:
    """Test AI orchestration functionality"""
    
    @pytest.mark.asyncio
    async def test_natural_language_query(self, orchestrator):
        """Test natural language query processing"""
        query = "What are the latest clinical trials for diabetes?"
        
        result = await orchestrator.process_natural_language_query(query)
        
        assert result is not None
        assert "domain" in result
        assert result["domain"] == "paragonrx"
        assert "results" in result
        
    @pytest.mark.asyncio
    async def test_web_scraping_team(self, orchestrator):
        """Test web scraping team coordination"""
        from core.agents.web_scraping_agents import FinanceWebScrapingTeam
        
        team = FinanceWebScrapingTeam()
        task = {
            "query": "AAPL stock analysis",
            "sources": ["yahoo_finance", "bloomberg"]
        }
        
        result = await team.coordinate_research(task)
        
        assert result is not None
        assert "aggregated_data" in result
        assert len(result["sources"]) == 2
        
    @pytest.mark.asyncio
    async def test_integration_specialists(self, orchestrator):
        """Test platform integration specialists"""
        from core.agents.integration_specialists import IntegrationCoordinator
        
        coordinator = IntegrationCoordinator()
        
        # Test with mocked integrations
        with patch.object(coordinator, 'gong_agent') as mock_gong:
            mock_gong.fetch_data = AsyncMock(return_value={"calls": []})
            
            result = await coordinator.gather_multi_platform_data(
                query="sales performance last quarter",
                platforms=["gong"]
            )
            
            assert result is not None
            assert "gong" in result
            
    @pytest.mark.asyncio
    async def test_circuit_breaker(self, orchestrator):
        """Test circuit breaker functionality"""
        from core.agents.unified_orchestrator import UnifiedOrchestrator
        
        unified = UnifiedOrchestrator()
        
        # Simulate failures to trigger circuit breaker
        with patch.object(unified.gong_breaker, '_call') as mock_call:
            mock_call.side_effect = Exception("API Error")
            
            # Should fail after threshold
            for _ in range(6):
                try:
                    await unified._safe_integration_call(
                        unified.gong_breaker,
                        AsyncMock(),
                        "test"
                    )
                except:
                    pass
                    
            # Circuit should be open
            assert unified.gong_breaker.current_state == "open"
            
    @pytest.mark.asyncio
    async def test_resource_limits(self):
        """Test resource management and limits"""
        from core.resource_management import ResourceManager, ResourceLimitedAgent
        
        manager = ResourceManager()
        manager.max_concurrent_agents = 2  # Low limit for testing
        
        # Create agents up to limit
        async with ResourceLimitedAgent("agent1") as agent1:
            async with ResourceLimitedAgent("agent2") as agent2:
                # Third agent should be queued
                assert manager.agent_queue.empty()
                
                # Try to create third agent
                try:
                    success = await manager.acquire_agent_slot("agent3")
                    assert not success  # Should be queued
                    assert manager.agent_queue.qsize() == 1
                except:
                    pass
                    
    @pytest.mark.asyncio
    async def test_input_validation(self):
        """Test input validation and sanitization"""
        from core.security.input_validation import InputValidator, QueryRequest
        
        # Test SQL injection detection
        malicious_input = "'; DROP TABLE users; --"
        with pytest.raises(ValueError, match="SQL injection"):
            InputValidator.sanitize_string(malicious_input)
            
        # Test XSS detection
        xss_input = "<script>alert('xss')</script>"
        with pytest.raises(ValueError, match="XSS"):
            InputValidator.sanitize_string(xss_input)
            
        # Test valid input
        valid_input = "What is the weather today?"
        sanitized = InputValidator.sanitize_string(valid_input)
        assert sanitized == valid_input
        
        # Test Pydantic validation
        with pytest.raises(ValueError):
            QueryRequest(query="test", max_results=200)  # Exceeds limit
            
    @pytest.mark.asyncio
    async def test_monitoring_alerts(self):
        """Test monitoring and alerting system"""
        from core.monitoring.enhanced_monitoring import AlertManager, MetricsCollector
        
        alert_manager = AlertManager()
        
        # Test alert triggering
        high_error_metrics = {
            "error_rate": 0.10,  # 10% error rate
            "response_time_p95": 1.0,
            "cpu_usage": 50
        }
        
        await alert_manager.check_alerts(high_error_metrics)
        
        assert len(alert_manager.alert_history) > 0
        assert alert_manager.alert_history[0]["type"] == "ERROR_RATE"
        
    @pytest.mark.asyncio
    async def test_end_to_end_flow(self, orchestrator):
        """Test complete end-to-end flow"""
        # Simulate user query
        query = "Show me competitor analysis for our product"
        
        # Process through orchestrator
        with patch.object(orchestrator, 'unified_orchestrator') as mock_unified:
            mock_unified.process_query = AsyncMock(return_value={
                "domain": "sophia",
                "confidence": 0.95,
                "results": [{"competitor": "CompanyA", "market_share": 0.25}]
            })
            
            result = await orchestrator.process_natural_language_query(query)
            
            assert result["domain"] == "sophia"
            assert len(result["results"]) > 0
            assert result["confidence"] > 0.9

@pytest.mark.asyncio
async def test_performance_benchmarks():
    """Performance benchmark tests"""
    from services.ai_agent_orchestrator import AIAgentOrchestrator
    import time
    
    orchestrator = AIAgentOrchestrator()
    
    # Measure query processing time
    start = time.time()
    
    tasks = []
    for i in range(10):
        task = orchestrator.process_natural_language_query(f"Test query {i}")
        tasks.append(task)
        
    results = await asyncio.gather(*tasks)
    
    end = time.time()
    avg_time = (end - start) / 10
    
    # Should meet performance target
    assert avg_time < 0.1  # 100ms target
    
    print(f"Average query time: {avg_time*1000:.2f}ms")
'''
            
            with open('tests/test_ai_orchestration_integration.py', 'w') as f:
                f.write(test_code)
                
            # Create tests __init__.py
            os.makedirs('tests', exist_ok=True)
            with open('tests/__init__.py', 'w') as f:
                f.write('')
                
            self.log_action("Create integration tests", "SUCCESS",
                          "Created comprehensive integration test suite")
            return True
            
        except Exception as e:
            self.log_action("Create integration tests", "FAILED", str(e))
            return False
            
    def create_deployment_scripts(self):
        """Create production deployment scripts"""
        try:
            # Enhanced deployment script
            deploy_script = '''#!/bin/bash
# Enhanced AI Orchestration Deployment Script

set -e  # Exit on error

echo "üöÄ Starting AI Orchestration Deployment..."

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

# Configuration
DEPLOYMENT_ENV=${1:-staging}
HEALTH_CHECK_RETRIES=30
HEALTH_CHECK_INTERVAL=10

# Functions
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

check_prerequisites() {
    log_info "Checking prerequisites..."
    
    # Check Python version
    python_version=$(python3 --version 2>&1 | awk '{print $2}')
    required_version="3.8"
    
    if ! python3 -c "import sys; exit(0 if sys.version_info >= (3,8) else 1)"; then
        log_error "Python 3.8+ required. Found: $python_version"
        exit 1
    fi
    
    # Check required services
    services=("postgresql" "redis" "docker")
    for service in "${services[@]}"; do
        if ! command -v $service &> /dev/null; then
            log_error "$service is not installed"
            exit 1
        fi
    done
    
    log_info "Prerequisites check passed ‚úì"
}

setup_environment() {
    log_info "Setting up environment..."
    
    # Create .env from example if not exists
    if [ ! -f .env ]; then
        if [ -f .env.example ]; then
            cp .env.example .env
            log_warn "Created .env from .env.example - please update with actual values"
        else
            log_error ".env file not found"
            exit 1
        fi
    fi
    
    # Load environment variables
    export $(cat .env | grep -v '^#' | xargs)
    
    # Validate required variables
    required_vars=("DATABASE_URL" "REDIS_URL" "WEAVIATE_URL")
    for var in "${required_vars[@]}"; do
        if [ -z "${!var}" ]; then
            log_error "Required environment variable $var is not set"
            exit 1
        fi
    done
}

install_dependencies() {
    log_info "Installing dependencies..."
    
    # Create virtual environment if not exists
    if [ ! -d "venv" ]; then
        python3 -m venv venv
    fi
    
    # Activate virtual environment
    source venv/bin/activate
    
    # Upgrade pip
    pip install --upgrade pip
    
    # Install requirements
    pip install -r requirements_ai_orchestration.txt
    
    log_info "Dependencies installed ‚úì"
}

run_migrations() {
    log_info "Running database migrations..."
    
    # Create migration script if needed
    cat > run_migrations.py << 'EOF'
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine
import os

async def create_tables():
    engine = create_async_engine(os.getenv("DATABASE_URL"))
    
    async with engine.begin() as conn:
        # Create tables
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS agents (
                id SERIAL PRIMARY KEY,
                domain VARCHAR(50) NOT NULL,
                status VARCHAR(20) DEFAULT 'active',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS tasks (
                id SERIAL PRIMARY KEY,
                agent_id INTEGER REFERENCES agents(id),
                status VARCHAR(20) DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                execution_time FLOAT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP
            )
        """)
        
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                id SERIAL PRIMARY KEY,
                agent_id INTEGER REFERENCES agents(id),
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                response_time FLOAT,
                error_code VARCHAR(50),
                request_type VARCHAR(50)
            )
        """)
        
        # Create indexes
        from core.performance_config import OPTIMIZED_INDEXES
        for index_sql in OPTIMIZED_INDEXES:
            try:
                await conn.execute(index_sql)
            except Exception as e:
                print(f"Index creation warning: {e}")
    
    await engine.dispose()
    print("Database setup complete")

asyncio.run(create_tables())
EOF
    
    python3 run_migrations.py
    rm run_migrations.py
    
    log_info "Database migrations complete ‚úì"
}

start_services() {
    log_info "Starting services..."
    
    # Start Weaviate if using Docker
    if [ "$DEPLOYMENT_ENV" != "production" ]; then
        if ! docker ps | grep -q weaviate; then
            log_info "Starting Weaviate..."
            docker run -d \
                --name weaviate \
                -p 8080:8080 \
                -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \
                -e PERSISTENCE_DATA_PATH=/var/lib/weaviate \
                semitechnologies/weaviate:latest
        fi
    fi
    
    # Start the orchestration service
    log_info "Starting AI Orchestration service..."
    
    if [ "$DEPLOYMENT_ENV" == "production" ]; then
        # Production: Use gunicorn with uvicorn workers
        gunicorn services.ai_agent_orchestrator:app \
            --workers 4 \
            --worker-class uvicorn.workers.UvicornWorker \
            --bind 0.0.0.0:8000 \
            --timeout 60 \
            --access-logfile - \
            --error-logfile - \
            --daemon
    else
        # Development/Staging: Use uvicorn directly
        uvicorn services.ai_agent_orchestrator:app \
            --host 0.0.0.0 \
            --port 8000 \
            --reload &
    fi
    
    SERVICE_PID=$!
    echo $SERVICE_PID > orchestration.pid
    
    log_info "Service started with PID: $SERVICE_PID"
}

health_check() {
    log_info "Performing health checks..."
    
    for i in $(seq 1 $HEALTH_CHECK_RETRIES); do
        if curl -f http://localhost:8000/health > /dev/null 2>&1; then
            log_info "Health check passed ‚úì"
            return 0
        fi
        
        log_warn "Health check attempt $i/$HEALTH_CHECK_RETRIES failed, retrying..."
        sleep $HEALTH_CHECK_INTERVAL
    done
    
    log_error "Health check failed after $HEALTH_CHECK_RETRIES attempts"
    return 1
}

run_tests() {
    log_info "Running integration tests..."
    
    # Run pytest with coverage
    pytest tests/test_ai_orchestration_integration.py \
        --cov=services \
        --cov=core \
        --cov-report=term-missing \
        --cov-report=html \
        -v
        
    if [ $? -eq 0 ]; then
        log_info "All tests passed ‚úì"
    else
        log_error "Tests failed"
        exit 1
    fi
}

# Main deployment flow
main() {
    log_info "Deployment environment: $DEPLOYMENT_ENV"
    
    check_prerequisites
    setup_environment
    install_dependencies
    run_migrations
    
    if [ "$DEPLOYMENT_ENV" != "production" ]; then
        run_tests
    fi
    
    start_services
    health_check
    
    if [ $? -eq 0 ]; then
        log_info "üéâ AI Orchestration deployment successful!"
        log_info "Service running at: http://localhost:8000"
        log_info "Health endpoint: http://localhost:8000/health"
        log_info "Metrics endpoint: http://localhost:8000/metrics"
    else
        log_error "Deployment failed"
        exit 1
    fi
}

# Run main function
main
'''
            
            with open('deploy_production.sh', 'w') as f:
                f.write(deploy_script)
                
            os.chmod('deploy_production.sh', 0o755)
            
            self.log_action("Create deployment scripts", "SUCCESS",
                          "Created production deployment script")
            return True
            
        except Exception as e:
            self.log_action("Create deployment scripts", "FAILED", str(e))
            return False
            
    def generate_report(self):
        """Generate comprehensive remediation report"""
        try:
            report = {
                "remediation_summary": {
                    "start_time": self.start_time.isoformat(),
                    "end_time": datetime.now().isoformat(),
                    "duration": str(datetime.now() - self.start_time),
                    "issues_fixed": len(self.issues_fixed),
                    "issues_pending": len(self.issues_pending),
                    "success_rate": len(self.issues_fixed) / (len(self.issues_fixed) + len(self.issues_pending)) * 100 if (len(self.issues_fixed) + len(self.issues_pending)) > 0 else 0
                },
                "fixed_issues": self.issues_fixed,
                "pending_issues": self.issues_pending,
                "test_results": self.test_results,
                "next_steps": [
                    "Review and update .env file with actual values",
                    "Run deployment script: ./deploy_production.sh staging",
                    "Monitor system health at http://localhost:8000/health",
                    "Review metrics at http://localhost:8000/metrics",
                    "Set up external monitoring (Datadog/New Relic)",
                    "Configure backup and disaster recovery",
                    "Perform load testing before production",
                    "Set up CI/CD pipeline"
                ],
                "production_checklist": {
                    "security": [
                        "Update all API keys and secrets",
                        "Enable HTTPS/TLS",
                        "Configure firewall rules",
                        "Set up rate limiting",
                        "Enable audit logging"
                    ],
                    "performance": [
                        "Tune database connection pool",
                        "Configure Redis persistence",
                        "Set up CDN for static assets",
                        "Enable response caching",
                        "Configure auto-scaling"
                    ],
                    "monitoring": [
                        "Set up Prometheus/Grafana",
                        "Configure alert webhooks",
                        "Enable distributed tracing",
                        "Set up log aggregation",
                        "Configure uptime monitoring"
                    ]
                }
            }
            
            with open('remediation_report.json', 'w') as f:
                json.dump(report, f, indent=2)
                
            self.log_action("Generate report", "SUCCESS",
                          "Created comprehensive remediation report")
            return True
            
        except Exception as e:
            self.log_action("Generate report", "FAILED", str(e))
            return False
            
    def run_remediation(self):
        """Execute all remediation steps"""
        logging.info("=" * 50)
        logging.info("AI ORCHESTRATION COMPREHENSIVE REMEDIATION")
        logging.info("=" * 50)
        
        # Execute remediation steps in order
        steps = [
            ("Fix core.monitoring __init__.py", self.fix_core_monitoring_init),
            ("Install dependencies", self.install_dependencies),
            ("Create environment config", self.create_environment_config),
            ("Add input validation", self.add_input_validation),
            ("Configure CORS", self.configure_cors),
            ("Add resource limits", self.add_resource_limits),
            ("Optimize performance", self.optimize_performance),
            ("Add monitoring/alerting", self.add_monitoring_alerting),
            ("Create integration tests", self.create_integration_tests),
            ("Create deployment scripts", self.create_deployment_scripts)
        ]
        
        for step_name, step_func in steps:
            logging.info(f"\nüîß Executing: {step_name}")
            success = step_func()
            
            if not success:
                logging.warning(f"‚ö†Ô∏è  Step '{step_name}' failed, continuing...")
                
        # Generate final report
        self.generate_report()
        
        # Print summary
        logging.info("\n" + "=" * 50)
        logging.info("REMEDIATION COMPLETE")
        logging.info("=" * 50)
        logging.info(f"‚úÖ Issues Fixed: {len(self.issues_fixed)}")
        logging.info(f"‚ùå Issues Pending: {len(self.issues_pending)}")
        logging.info(f"üìä Success Rate: {len(self.issues_fixed) / (len(self.issues_fixed) + len(self.issues_pending)) * 100:.1f}%")
        logging.info(f"üìÑ Detailed report: remediation_report.json")
        logging.info(f"üìù Remediation log: remediation_log.txt")
        
        # Run validation test again
        logging.info("\nüîç Running validation test to verify fixes...")
        result = subprocess.run(
            [sys.executable, "test_ai_orchestration_deployment.py"],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            logging.info("‚úÖ Validation test passed!")
        else:
            logging.warning("‚ö†Ô∏è  Some validation issues remain - check the output")
            
        return len(self.issues_pending) == 0

if __name__ == "__main__":
    remediator = AIOrchestrationRemediator()
    success = remediator.run_remediation()
    
    if success:
        print("\n‚úÖ All issues successfully remediated!")
        print("Next step: Run './deploy_production.sh staging' to deploy")
    else:
        print("\n‚ö†Ô∏è  Some issues require manual intervention")
        print("Check remediation_report.json for details")
        
    sys.exit(0 if success else 1)