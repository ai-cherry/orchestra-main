steps:
# Setup Python and dependencies
- name: 'python:3.11-slim'
  id: 'install-dependencies'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      pip install poetry
      poetry install
      
# Run linting and formatting checks
- name: 'python:3.11-slim'
  id: 'linting'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      pip install poetry
      poetry install
      poetry add black isort flake8
      echo "Running code formatting check with black..."
      poetry run black --check . || echo "Warning: Formatting check with black failed. Please format your code."
      echo "Running import sorting check with isort..."
      poetry run isort --check-only --diff . || echo "Warning: Import sorting check with isort failed. Please sort your imports."
      echo "Running linting check with flake8..."
      poetry run flake8 . || echo "Warning: Linting check with flake8 failed. Please fix the issues reported by flake8."
      # We're making these non-blocking for debugging purposes, similar to the GitHub workflow

# Run critical tests
- name: 'python:3.11-slim'
  id: 'critical-tests'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      pip install poetry
      poetry install
      export PYTHONPATH=$(pwd)
      poetry run pytest tests/ -m critical -v
  # This will fail the build if tests fail because we don't specify allowFailure: true

# Pre-deployment Gemini code analysis using Vertex AI
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
  id: 'gemini-code-analysis'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      echo "Running pre-deployment Gemini code analysis..."
      gcloud auth configure-docker
      
      # Set up Python
      apt-get update && apt-get install -y python3-pip
      pip3 install google-cloud-aiplatform pyyaml
      
      # Create a Python script to run Gemini analysis
      cat > gemini_analyzer.py << 'EOF'
      import os
      import sys
      import yaml
      import glob
      from google.cloud import aiplatform
      
      # Initialize Vertex AI with the project and region
      aiplatform.init(project='cherry-ai-project', location='us-west4')
      
      # Load prompt templates
      with open('templates/gemini_prompts.yaml', 'r') as f:
          prompts = yaml.safe_load(f)['prompts']
      
      # Function to analyze code with Gemini
      def analyze_code(file_pattern, prompt_key='fix_code'):
          prompt_template = prompts[prompt_key]['context']
          
          # Find files matching the pattern
          files = glob.glob(file_pattern, recursive=True)
          if not files:
              print(f"No files found matching pattern: {file_pattern}")
              return
          
          # Process each file
          for file_path in files:
              try:
                  with open(file_path, 'r') as f:
                      code_content = f.read()
                  
                  if not code_content.strip():
                      print(f"File {file_path} is empty, skipping.")
                      continue
                  
                  print(f"Analyzing {file_path}...")
                  
                  # Create the prompt
                  prompt = f"{prompt_template}\n\nFile: {file_path}\n\n```\n{code_content}\n```\n\nAnalysis:"
                  
                  # Call Gemini model
                  model = aiplatform.VertexAI(
                      model_name="gemini-pro",
                      max_output_tokens=1024,
                      temperature=0,
                  )
                  
                  response = model.predict(prompt)
                  
                  print(f"\n--- Analysis for {file_path} ---")
                  print(response.text)
                  print(f"--- End of analysis for {file_path} ---\n")
                  
              except Exception as e:
                  print(f"Error analyzing {file_path}: {str(e)}")
      
      # Analyze key files
      analyze_code("core/orchestrator/src/**/*.py")
      analyze_code("agent/**/*.py")
      analyze_code("packages/**/*.py")
      analyze_code("Dockerfile")
      EOF
      
      # Run the analysis
      python3 gemini_analyzer.py

# Build and push Docker image
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'us-west4-docker.pkg.dev/cherry-ai.me/orchestra/api:$_ENV', '.']

- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'us-west4-docker.pkg.dev/cherry-ai.me/orchestra/api:$_ENV']

# Scan Docker image for vulnerabilities
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'vulnerability-scan'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      echo "Scanning Docker image for vulnerabilities..."
      gcloud artifacts docker images scan \
        us-docker.pkg.dev/cherry-ai.me/orchestra/app:latest \
        --format=json | tee scan-results.json
      
      # Check for critical/high vulnerabilities and fail the build if found
      CRITICAL_COUNT=$(grep -c '"severity": "CRITICAL"' scan-results.json || true)
      HIGH_COUNT=$(grep -c '"severity": "HIGH"' scan-results.json || true)
      
      echo "Found $CRITICAL_COUNT critical and $HIGH_COUNT high severity vulnerabilities"
      
      if [ "$CRITICAL_COUNT" -gt 0 ] || [ "$HIGH_COUNT" -gt 3 ]; then
        echo "Security scan failed due to too many critical/high vulnerabilities"
        exit 1
      fi

# Deploy to Cloud Run with secrets from Secret Manager
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: gcloud
  args:
  - 'run'
  - 'deploy'
  - 'orchestra-$_ENV'
  - '--image'
  - 'us-west4-docker.pkg.dev/cherry-ai.me/orchestra/api:$_ENV'
  - '--region'
  - '$_REGION'
  - '--platform'
  - 'managed'
  - '--project'
  - 'cherry-ai.me'
  - '--service-account'
  - 'vertex-agent@cherry-ai.me.iam.gserviceaccount.com'

# Available substitution variables:
# $_PROJECT_ID: the Google Cloud project ID
# $COMMIT_SHA: the commit SHA being built
# $REVISION_ID: a unique ID for the build
# $REPO_NAME: the repository name
# $BRANCH_NAME: the branch or tag name

# Default timeout is 10 minutes
timeout: 1800s

# Use the 'cherry-ai.me' project
options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'
  env:
    - 'PYTHONUNBUFFERED=1'
  substitution_option: 'ALLOW_LOOSE'

# Use a service account with the necessary permissions
serviceAccount: 'projects/525398941159/serviceAccounts/vertex-agent@cherry-ai.me.iam.gserviceaccount.com'

# Artifacts configuration for storing build outputs
artifacts:
  objects:
    location: 'gs://cherry-ai-me-cloudbuild-artifacts/'
    paths: ['scan-results.json']

substitutions:
  _ENV: 'prod'
  _REGION: 'us-west4'

images:
- 'us-west4-docker.pkg.dev/cherry-ai.me/orchestra/api:$_ENV'
